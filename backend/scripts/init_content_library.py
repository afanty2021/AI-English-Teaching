#!/usr/bin/env python
"""
å†…å®¹åº“åˆå§‹åŒ–CLIå·¥å…· - AIè‹±è¯­æ•™å­¦ç³»ç»Ÿ

æä¾›å†…å®¹åº“åˆå§‹åŒ–ã€å¯¼å…¥ã€ç´¢å¼•ã€å»é‡ã€éªŒè¯å’Œç»Ÿè®¡åŠŸèƒ½

ç”¨æ³•:
    python scripts/init_content_library.py init --all          # å®Œæ•´åˆå§‹åŒ–
    python scripts/init_content_library.py import --content data/contents/
    python scripts/init_content_library.py index --batch-size 50
    python scripts/init_content_library.py deduplicate         # è¯­ä¹‰å»é‡
    python scripts/init_content_library.py validate --content data/contents/
    python scripts/init_content_library.py stats              # æŸ¥çœ‹ç»Ÿè®¡
    python scripts/init_content_library.py update --file new_contents.json  # å¢é‡æ›´æ–°
"""
import asyncio
import argparse
import json
import logging
import sys
from pathlib import Path
from typing import Optional

# æ·»åŠ backendåˆ°è·¯å¾„
backend_path = Path(__file__).parent.parent
sys.path.insert(0, str(backend_path))

from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import settings
from app.db.session import async_session
from app.services.content_import_service import ContentImportService, VocabularyImportService
from app.services.vector_service import VectorService
from app.services.content_deduplication_service import (
    ContentDeduplicationService,
    VocabularyDeduplicationService
)
from app.utils.content_validators import get_content_validator, get_vocabulary_validator

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="å†…å®¹åº“åˆå§‹åŒ–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # initå‘½ä»¤
    init_parser = subparsers.add_parser("init", help="å®Œæ•´åˆå§‹åŒ–")
    init_parser.add_argument("--content", type=str, help="å†…å®¹JSONæ–‡ä»¶/ç›®å½•")
    init_parser.add_argument("--vocabulary", type=str, help="è¯æ±‡JSONæ–‡ä»¶/ç›®å½•")
    init_parser.add_argument("--skip-index", action="store_true", help="è·³è¿‡å‘é‡ç´¢å¼•")
    init_parser.add_argument("--skip-duplicates", action="store_true", help="è·³è¿‡å»é‡")

    # importå‘½ä»¤
    import_parser = subparsers.add_parser("import", help="å¯¼å…¥å†…å®¹")
    import_parser.add_argument("--content", type=str, help="å†…å®¹JSONæ–‡ä»¶/ç›®å½•")
    import_parser.add_argument("--vocabulary", type=str, help="è¯æ±‡JSONæ–‡ä»¶/ç›®å½•")
    import_parser.add_argument("--skip-duplicates", action="store_true", help="è·³è¿‡å»é‡")

    # indexå‘½ä»¤
    index_parser = subparsers.add_parser("index", help="å‘é‡ç´¢å¼•")
    index_parser.add_argument("--batch-size", type=int, default=50, help="æ‰¹å¤„ç†å¤§å°")
    index_parser.add_argument("--content-id", type=str, action="append", help="æŒ‡å®šå†…å®¹ID")
    index_parser.add_argument("--all", action="store_true", help="ç´¢å¼•æ‰€æœ‰æœªç´¢å¼•çš„å†…å®¹")

    # deduplicateå‘½ä»¤
    deduplicate_parser = subparsers.add_parser("deduplicate", help="æ‰§è¡Œè¯­ä¹‰å»é‡")
    deduplicate_parser.add_argument("--content", action="store_true", help="å¯¹å†…å®¹å»é‡")
    deduplicate_parser.add_argument("--vocabulary", action="store_true", help="å¯¹è¯æ±‡å»é‡")
    deduplicate_parser.add_argument("--threshold", type=float, default=0.85, help="ç›¸ä¼¼åº¦é˜ˆå€¼")

    # validateå‘½ä»¤
    validate_parser = subparsers.add_parser("validate", help="éªŒè¯æ•°æ®")
    validate_parser.add_argument("--content", type=str, help="å†…å®¹JSONæ–‡ä»¶/ç›®å½•")
    validate_parser.add_argument("--vocabulary", type=str, help="è¯æ±‡JSONæ–‡ä»¶/ç›®å½•")

    # statså‘½ä»¤
    subparsers.add_parser("stats", help="æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯")

    # updateå‘½ä»¤
    update_parser = subparsers.add_parser("update", help="å¢é‡æ›´æ–°")
    update_parser.add_argument("--file", type=str, required=True, help="æ–°å†…å®¹JSONæ–‡ä»¶")
    update_parser.add_argument("--index", action="store_true", help="åŒæ—¶ç´¢å¼•æ–°å†…å®¹")

    return parser.parse_args()


async def progress_callback(phase: str, current: int, total: int, message: str):
    """è¿›åº¦å›è°ƒ"""
    if total == 0:
        print(f"\r{phase:12s} - {message}", end="", flush=True)
        return

    percent = (current / total) * 100
    bar_length = 30
    filled = int(bar_length * percent // 100)
    bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
    print(f"\r{phase:12s} |{bar}| {current}/{total} ({percent:.1f}%) - {message}", end="", flush=True)
    if phase == "complete":
        print()


async def cmd_init(args):
    """initå‘½ä»¤å¤„ç†"""
    print("Starting content library initialization...")

    async with async_session() as db:
        vector_service = VectorService()
        import_service = ContentImportService(db, vector_service)

        # å¯¼å…¥å†…å®¹
        if args.content:
            success, failed = await import_service.import_from_file(
                args.content, not args.skip_duplicates
            )
            print(f"\nContent import: {success} success, {failed} failed")

        # å¯¼å…¥è¯æ±‡
        if args.vocabulary:
            vocab_service = VocabularyImportService(db, vector_service)
            success, failed = await vocab_service.import_from_file(
                args.vocabulary, not args.skip_duplicates
            )
            print(f"\nVocabulary import: {success} success, {failed} failed")

        # ç´¢å¼•å‘é‡
        if not args.skip_index:
            print("\nIndexing vectors...")
            stats = await vector_service.batch_index_contents(
                db, batch_size=50, progress_callback=progress_callback
            )
            print(f"\nIndex results: {stats}")


async def cmd_import(args):
    """importå‘½ä»¤å¤„ç†"""
    async with async_session() as db:
        vector_service = VectorService()

        if args.content:
            import_service = ContentImportService(db, vector_service)
            success, failed = await import_service.import_from_file(
                args.content, not args.skip_duplicates
            )
            print(f"Content import: {success} success, {failed} failed")

        if args.vocabulary:
            vocab_service = VocabularyImportService(db, vector_service)
            success, failed = await vocab_service.import_from_file(
                args.vocabulary, not args.skip_duplicates
            )
            print(f"Vocabulary import: {success} success, {failed} failed")


async def cmd_index(args):
    """indexå‘½ä»¤å¤„ç†"""
    import uuid

    async with async_session() as db:
        vector_service = VectorService()

        content_ids = None
        if args.content_id:
            content_ids = [uuid.UUID(id) for id in args.content_id]

        if not args.content_id and not args.all:
            print("Please specify --content-id or --all")
            return

        print("Indexing vectors...")
        stats = await vector_service.batch_index_contents(
            db,
            content_ids=content_ids,
            batch_size=args.batch_size,
            progress_callback=progress_callback
        )
        print(f"\nIndex results: {stats}")


async def cmd_deduplicate(args):
    """deduplicateå‘½ä»¤å¤„ç†"""
    if not args.content and not args.vocabulary:
        print("Please specify --content and/or --vocabulary")
        return

    if args.content:
        print("Deduplicating contents...")
        deduplication_service = ContentDeduplicationService(
            similarity_threshold=args.threshold
        )

        # åŠ è½½ç°æœ‰å†…å®¹
        from app.models.content import Content
        from sqlalchemy import select

        async with async_session() as db:
            result = await db.execute(select(Content))
            contents = result.scalars().all()

            if not contents:
                print("No contents found to deduplicate")
                return

            contents_data = [
                {
                    "id": str(c.id),
                    "title": c.title,
                    "description": c.description,
                    "content_text": c.content_text,
                    "topic": c.topic,
                    "tags": c.tags or [],
                }
                for c in contents
            ]

            duplicates = await deduplication_service.find_duplicates(
                [c["title"] for c in contents_data],
                progress_callback
            )

            if duplicates:
                print(f"\nFound {len(duplicates)} duplicate groups:")
                for i, group in enumerate(duplicates):
                    print(f"  Group {i + 1}: {[contents_data[j]['title'] for j in group]}")
            else:
                print("No duplicates found")

    if args.vocabulary:
        print("\nDeduplicating vocabularies...")
        deduplication_service = VocabularyDeduplicationService(
            similarity_threshold=args.threshold
        )

        from app.models.content import Vocabulary
        from sqlalchemy import select

        async with async_session() as db:
            result = await db.execute(select(Vocabulary))
            vocabularies = result.scalars().all()

            if not vocabularies:
                print("No vocabularies found to deduplicate")
                return

            duplicates = await deduplication_service.find_duplicates([
                {"word": v.word, "definitions": v.definitions or []}
                for v in vocabularies
            ])

            if duplicates:
                print(f"\nFound {len(duplicates)} duplicate groups:")
                for i, group in enumerate(duplicates):
                    print(f"  Group {i + 1}: {[vocabularies[j].word for j in group]}")
            else:
                print("No duplicates found")


async def cmd_validate(args):
    """validateå‘½ä»¤å¤„ç†"""
    if not args.content and not args.vocabulary:
        print("Please specify --content and/or --vocabulary")
        return

    if args.content:
        path = Path(args.content)
        if path.is_file():
            validator = get_content_validator()
            success, errors = validator.validate_file(str(path))
            print(f"Validation of {path}: {'PASSED' if success else 'FAILED'}")
            if errors:
                for error in errors:
                    print(f"  - {error}")
        elif path.is_dir():
            print(f"Validating all JSON files in {path}...")
            json_files = list(path.glob("**/*.json"))
            passed = 0
            failed = 0
            for json_file in json_files:
                validator = get_content_validator()
                success, errors = validator.validate_file(str(json_file))
                if success:
                    passed += 1
                    print(f"  âœ“ {json_file.relative_to(path)}")
                else:
                    failed += 1
                    print(f"  âœ— {json_file.relative_to(path)}: {errors[0]}")
            print(f"\nResults: {passed} passed, {failed} failed")

    if args.vocabulary:
        path = Path(args.vocabulary)
        if path.is_file():
            validator = get_vocabulary_validator()
            success, errors = validator.validate_file(str(path))
            print(f"Validation of {path}: {'PASSED' if success else 'FAILED'}")
            if errors:
                for error in errors:
                    print(f"  - {error}")
        elif path.is_dir():
            print(f"Validating all JSON files in {path}...")
            json_files = list(path.glob("**/*.json"))
            passed = 0
            failed = 0
            for json_file in json_files:
                validator = get_vocabulary_validator()
                success, errors = validator.validate_file(str(json_file))
                if success:
                    passed += 1
                    print(f"  âœ“ {json_file.relative_to(path)}")
                else:
                    failed += 1
                    print(f"  âœ— {json_file.relative_to(path)}: {errors[0]}")
            print(f"\nResults: {passed} passed, {failed} failed")


async def cmd_stats(args):
    """statså‘½ä»¤å¤„ç†"""
    async with async_session() as db:
        import_service = ContentImportService(db)
        stats = await import_service.get_import_stats()

        print("\n=== Content Library Statistics ===")
        print("\nğŸ“š Contents by Type:")
        for content_type, count in stats.items():
            if content_type != '_total':
                print(f"  {content_type}: {count}")
        print(f"\n  Total: {stats.get('_total', 0)}")

        # è¯æ±‡ç»Ÿè®¡
        from app.models.content import Vocabulary
        from sqlalchemy import select, func

        result = await db.execute(select(func.count(Vocabulary.id)))
        vocab_count = result.scalar()
        print(f"\nğŸ“– Vocabularies: {vocab_count}")


async def cmd_update(args):
    """updateå‘½ä»¤å¤„ç†"""
    async with async_session() as db:
        import_service = ContentImportService(db)

        success, failed = await import_service.import_from_file(args.file)
        print(f"Update import: {success} success, {failed} failed")

        if args.index:
            vector_service = VectorService()
            stats = await vector_service.batch_index_contents(
                db, batch_size=50, progress_callback=progress_callback
            )
            print(f"Index results: {stats}")


async def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    if not args.command:
        print("Please specify a command. Use --help for usage.")
        sys.exit(1)

    commands = {
        "init": cmd_init,
        "import": cmd_import,
        "index": cmd_index,
        "deduplicate": cmd_deduplicate,
        "validate": cmd_validate,
        "stats": cmd_stats,
        "update": cmd_update,
    }

    command = commands.get(args.command)
    if command:
        await command(args)
    else:
        print(f"Unknown command: {args.command}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
