# è¯­éŸ³è¯†åˆ«é›†æˆä¼˜åŒ–è®¡åˆ’

> **åˆ¶å®šæ—¶é—´**: 2026-02-05
> **ç›®æ ‡**: æå‡è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡ã€å…¼å®¹æ€§å’Œç”¨æˆ·ä½“éªŒ
> **ä¼˜å…ˆçº§**: é«˜ (MVPå‘å¸ƒå‰å¿…é¡»å®Œæˆ)

---

## ğŸ“Š å½“å‰çŠ¶å†µåˆ†æ

### âœ… å·²å®ç°åŠŸèƒ½
- **åŸºç¡€Web Speech APIé›†æˆ**: å®Œæ•´çš„TypeScriptå°è£…
- **çŠ¶æ€ç®¡ç†**: å®Œæ•´çš„ç”Ÿå‘½å‘¨æœŸå’ŒçŠ¶æ€è¿½è¸ª
- **é”™è¯¯å¤„ç†**: å…¨é¢çš„é”™è¯¯ç±»å‹å¤„ç†å’Œç”¨æˆ·æç¤º
- **å•å…ƒæµ‹è¯•**: 394è¡Œæµ‹è¯•ä»£ç ï¼Œ100%è¦†ç›–ç‡
- **å‰ç«¯é›†æˆ**: ConversationView.vueä¸­å®Œæ•´é›†æˆä½¿ç”¨

### âŒ å¾…ä¼˜åŒ–é—®é¢˜
1. **æµè§ˆå™¨å…¼å®¹æ€§å·®**: ä»…æ”¯æŒChrome/Edgeï¼Œå…¶ä»–æµè§ˆå™¨ç”¨æˆ·ä½“éªŒå·®
2. **è¯†åˆ«å‡†ç¡®ç‡ä½**: å°¤å…¶åœ¨å˜ˆæ‚ç¯å¢ƒæˆ–æ–¹è¨€å£éŸ³ä¸‹
3. **å®æ—¶å¯¹è¯å»¶è¿Ÿ**: è¯­éŸ³è¯†åˆ«åˆ°ç»“æœæ˜¾ç¤ºæœ‰å»¶è¿Ÿ
4. **å¤šè¯­è¨€æ”¯æŒä¸è¶³**: ä»…æ”¯æŒè‹±è¯­å’Œä¸­æ–‡ï¼Œå…¶ä»–è¯­è¨€ç¼ºå¤±
5. **å™ªéŸ³è¿‡æ»¤ç¼ºå¤±**: æ²¡æœ‰è¯­éŸ³æ´»åŠ¨æ£€æµ‹å’Œå™ªéŸ³æŠ‘åˆ¶
6. **ç¦»çº¿æ”¯æŒç¼ºå¤±**: æ— ç½‘ç»œæ—¶å®Œå…¨æ— æ³•ä½¿ç”¨
7. **è¯­éŸ³è´¨é‡æ£€æµ‹**: ç¼ºä¹éº¦å…‹é£æƒé™å’ŒéŸ³é‡æ£€æµ‹

---

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

### æ€§èƒ½æŒ‡æ ‡
- **å‡†ç¡®ç‡æå‡**: ä»å½“å‰70%æå‡è‡³85%ä»¥ä¸Š
- **å“åº”æ—¶é—´**: è¯­éŸ³è¯†åˆ«å»¶è¿Ÿä»500msé™ä½è‡³200ms
- **æµè§ˆå™¨æ”¯æŒ**: è¦†ç›–ç‡è¾¾95%ä»¥ä¸Šï¼ˆChromeã€Firefoxã€Safariã€Edgeï¼‰
- **å¤šè¯­è¨€æ”¯æŒ**: æ–°å¢æ”¯æŒæ—¥è¯­ã€éŸ©è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­
- **ç¨³å®šæ€§**: å´©æºƒç‡é™ä½è‡³1%ä»¥ä¸‹

### ç”¨æˆ·ä½“éªŒæŒ‡æ ‡
- **ä¸€é”®å¼€å§‹**: ç®€åŒ–æ“ä½œæµç¨‹
- **å®æ—¶åé¦ˆ**: æ˜¾ç¤ºè¯†åˆ«çŠ¶æ€å’Œè¿›åº¦
- **é”™è¯¯æ¢å¤**: è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯æç¤ºä¼˜åŒ–
- **æ— éšœç¢æ”¯æŒ**: æ”¯æŒé”®ç›˜æ“ä½œå’Œå±å¹•é˜…è¯»å™¨

---

## ğŸ“ ä¼˜åŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: å¤šå±‚é™çº§ç­–ç•¥ (æ¨è)

#### 1.1 ä¼˜å…ˆçº§è¯†åˆ«å¼•æ“
```typescript
// è¯­éŸ³è¯†åˆ«ä¼˜å…ˆçº§é˜Ÿåˆ—
1. Web Speech API (æœ€é«˜ä¼˜å…ˆçº§)
   â”œâ”€â”€ Chrome/Edge: åŸç”Ÿæ”¯æŒ
   â”œâ”€â”€ Firefox: Polyfillæ”¯æŒ
   â””â”€â”€ Safari: é™çº§å¤„ç†

2. äº‘ç«¯STTæœåŠ¡ (ä¸­ç­‰ä¼˜å…ˆçº§)
   â”œâ”€â”€ OpenAI Whisper API
   â”œâ”€â”€ Google Cloud Speech
   â””â”€â”€ Azure Speech Services

3. ç¦»çº¿è¯†åˆ«å¼•æ“ (æœ€ä½ä¼˜å…ˆçº§)
   â”œâ”€â”€ Vosk WASM (è½»é‡çº§)
   â”œâ”€â”€ PocketSphinx.js
   â””â”€â”€ æµè§ˆå™¨åŸç”ŸAPI
```

#### 1.2 æ™ºèƒ½é™çº§æœºåˆ¶
```typescript
class VoiceRecognitionOptimizer {
  // 1. æµè§ˆå™¨èƒ½åŠ›æ£€æµ‹
  detectBrowserCapabilities(): BrowserCapability {
    return {
      webSpeechApi: checkWebSpeechAPI(),
      serviceWorker: checkServiceWorker(),
      wasm: checkWasmSupport(),
      bandwidth: measureNetworkBandwidth()
    }
  }

  // 2. è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¼•æ“
  selectBestEngine(capabilities: BrowserCapability): RecognitionEngine {
    if (capabilities.webSpeechApi && isChrome()) {
      return new WebSpeechEngine()
    } else if (capabilities.bandwidth > 1000) {
      return new CloudSTTEngine()
    } else {
      return new OfflineEngine()
    }
  }

  // 3. å®æ—¶æ€§èƒ½ç›‘æ§
  monitorPerformance(): void {
    this.trackAccuracy()
    this.trackLatency()
    this.trackErrorRate()
  }
}
```

#### 1.3 è¯­éŸ³è´¨é‡å¢å¼º
```typescript
class AudioEnhancer {
  // è¯­éŸ³æ´»åŠ¨æ£€æµ‹ (VAD)
  detectVoiceActivity(audioStream: MediaStream): Promise<boolean> {
    return new Promise((resolve) => {
      const audioContext = new AudioContext()
      const analyser = audioContext.createAnalyser()
      // VADç®—æ³•å®ç°
      resolve(hasVoiceActivity)
    })
  }

  // å™ªéŸ³æŠ‘åˆ¶
  noiseReduction(audioStream: MediaStream): MediaStream {
    // ä½¿ç”¨Web Audio APIè¿›è¡Œå™ªéŸ³æŠ‘åˆ¶
    const filteredStream = applyNoiseReductionFilter(audioStream)
    return filteredStream
  }

  // éŸ³é‡æ£€æµ‹
  measureVolume(audioStream: MediaStream): number {
    return getCurrentVolumeLevel(audioStream)
  }
}
```

### æ–¹æ¡ˆ2: äº‘ç«¯å¢å¼ºæ–¹æ¡ˆ

#### 2.1 OpenAI Whisperé›†æˆ
```python
# backend/app/services/speech_to_text_service.py
import openai
from fastapi import APIRouter, UploadFile, File
from typing import Optional

class SpeechToTextService:
    def __init__(self):
        self.openai_client = openai.AsyncOpenAI()

    async def transcribe_audio(
        self,
        audio_file: UploadFile,
        language: str = "en",
        model: str = "whisper-1"
    ) -> dict:
        """ä½¿ç”¨OpenAI Whisperè½¬å½•éŸ³é¢‘"""
        try:
            response = await self.openai_client.audio.transcriptions.create(
                model=model,
                file=audio_file,
                language=language,
                response_format="verbose_json",
                timestamp_granularities=["word"]
            )
            return {
                "text": response.text,
                "confidence": response.confidence,
                "words": response.words,
                "language": response.language
            }
        except Exception as e:
            raise Exception(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
```

#### 2.2 ç¼“å­˜æœºåˆ¶
```typescript
class STTCache {
  private cache = new Map<string, STTResult>()

  async getCachedResult(audioHash: string): Promise<STTResult | null> {
    return this.cache.get(audioHash) || null
  }

  setCachedResult(audioHash: string, result: STTResult): void {
    // LRUç¼“å­˜ï¼Œé™åˆ¶1000æ¡è®°å½•
    if (this.cache.size >= 1000) {
      const firstKey = this.cache.keys().next().value
      this.cache.delete(firstKey)
    }
    this.cache.set(audioHash, result)
  }
}
```

### æ–¹æ¡ˆ3: ç¦»çº¿è¯†åˆ«æ–¹æ¡ˆ

#### 3.1 Vosk WASMé›†æˆ
```typescript
// ä½¿ç”¨Vosk WASMå®ç°ç¦»çº¿è¯­éŸ³è¯†åˆ«
class OfflineVoiceRecognition {
  private vosk: Vosk | null = null

  async initialize(): Promise<void> {
    // åŠ è½½Vosk WASMæ¨¡å—
    this.vosk = await Vosk.create({
      modelPath: '/models/vosk-model-en-us-0.22/',
      wasmPath: '/libs/vosk/'
    })
  }

  async transcribe(audioData: Float32Array): Promise<string> {
    if (!this.vosk) {
      throw new Error('Voskæœªåˆå§‹åŒ–')
    }

    const result = await this.vosk.recognize(audioData)
    return result.text
  }
}
```

---

## ğŸ› ï¸ å®æ–½è®¡åˆ’

### Phase 1: åŸºç¡€ä¼˜åŒ– (3å¤©)

#### Day 1: æµè§ˆå™¨å…¼å®¹æ€§å¢å¼º
**ä»»åŠ¡**:
- [ ] å®ç°Firefox Polyfillæ”¯æŒ
- [ ] æ·»åŠ Safarié™çº§å¤„ç†
- [ ] åˆ›å»ºæµè§ˆå™¨èƒ½åŠ›æ£€æµ‹å‡½æ•°
- [ ] ä¼˜åŒ–é”™è¯¯æç¤ºä¿¡æ¯

**ä»£ç ç¤ºä¾‹**:
```typescript
// utils/browserCompatibility.ts
export class BrowserCompatibility {
  static detect(): BrowserInfo {
    const ua = navigator.userAgent
    const isChrome = /Chrome/.test(ua) && /Google Inc/.test(navigator.vendor)
    const isFirefox = /Firefox/.test(ua)
    const isSafari = /Safari/.test(ua) && /Apple Computer/.test(navigator.vendor)
    const isEdge = /Edg/.test(ua)

    return {
      engine: isChrome ? 'chrome' : isFirefox ? 'firefox' : isSafari ? 'safari' : 'unknown',
      version: this.getVersion(),
      webSpeechSupported: this.checkWebSpeechSupport()
    }
  }

  private static checkWebSpeechSupport(): boolean {
    return !!(window.SpeechRecognition || window.webkitSpeechRecognition)
  }
}
```

#### Day 2: è¯­éŸ³è´¨é‡å¢å¼º
**ä»»åŠ¡**:
- [ ] å®ç°è¯­éŸ³æ´»åŠ¨æ£€æµ‹ (VAD)
- [ ] æ·»åŠ å™ªéŸ³æŠ‘åˆ¶åŠŸèƒ½
- [ ] å®ç°éŸ³é‡å®æ—¶æ˜¾ç¤º
- [ ] ä¼˜åŒ–éŸ³é¢‘é‡‡é›†å‚æ•°

**ä»£ç ç¤ºä¾‹**:
```typescript
// utils/audioEnhancer.ts
export class AudioEnhancer {
  private audioContext: AudioContext
  private analyser: AnalyserNode

  constructor() {
    this.audioContext = new AudioContext()
    this.analyser = this.audioContext.createAnalyser()
  }

  async enhanceStream(stream: MediaStream): Promise<MediaStream> {
    // åˆ›å»ºå™ªéŸ³æŠ‘åˆ¶èŠ‚ç‚¹
    const noiseSuppressor = this.audioContext.createDynamicsCompressor()
    noiseSuppressor.threshold.setValueAtTime(-50, this.audioContext.currentTime)
    noiseSuppressor.knee.setValueAtTime(40, this.audioContext.currentTime)
    noiseSuppressor.ratio.setValueAtTime(12, this.audioContext.currentTime)

    // è¿æ¥éŸ³é¢‘å¤„ç†å›¾
    const source = this.audioContext.createMediaStreamSource(stream)
    source.connect(noiseSuppressor)

    return stream
  }

  detectVoiceActivity(stream: MediaStream): Promise<boolean> {
    return new Promise((resolve) => {
      const source = this.audioContext.createMediaStreamSource(stream)
      const analyser = this.audioContext.createAnalyser()
      analyser.fftSize = 512

      source.connect(analyser)
      const dataArray = new Uint8Array(analyser.frequencyBinCount)

      const detect = () => {
        analyser.getByteFrequencyData(dataArray)
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length

        // é˜ˆå€¼åˆ¤æ–­ (å¯è°ƒæ•´)
        resolve(average > 30)
      }

      detect()
    })
  }
}
```

#### Day 3: æ€§èƒ½ç›‘æ§å’Œç¼“å­˜
**ä»»åŠ¡**:
- [ ] å®ç°è¯†åˆ«å‡†ç¡®ç‡è¿½è¸ª
- [ ] æ·»åŠ å»¶è¿Ÿç›‘æ§
- [ ] åˆ›å»ºç»“æœç¼“å­˜æœºåˆ¶
- [ ] ä¼˜åŒ–çŠ¶æ€ç®¡ç†

**ä»£ç ç¤ºä¾‹**:
```typescript
// utils/performanceMonitor.ts
export class PerformanceMonitor {
  private metrics = {
    accuracy: 0,
    latency: 0,
    errorRate: 0,
    usageCount: 0
  }

  trackRecognition(
    startTime: number,
    endTime: number,
    isSuccessful: boolean,
    confidence: number
  ): void {
    this.metrics.latency = endTime - startTime
    this.metrics.usageCount++

    if (isSuccessful) {
      this.metrics.accuracy = (this.metrics.accuracy * (this.metrics.usageCount - 1) + confidence) / this.metrics.usageCount
    } else {
      this.metrics.errorRate = (this.metrics.errorRate * (this.metrics.usageCount - 1) + 1) / this.metrics.usageCount
    }
  }

  getMetrics(): RecognitionMetrics {
    return { ...this.metrics }
  }
}
```

### Phase 2: é«˜çº§åŠŸèƒ½ (4å¤©)

#### Day 4-5: äº‘ç«¯STTé›†æˆ
**ä»»åŠ¡**:
- [ ] é›†æˆOpenAI Whisper API
- [ ] å®ç°éŸ³é¢‘æ–‡ä»¶ä¸Šä¼ 
- [ ] æ·»åŠ å¤šè¯­è¨€æ”¯æŒ
- [ ] å®ç°ç»“æœè§£æå’Œåå¤„ç†

**ä»£ç ç¤ºä¾‹**:
```typescript
// api/speechToText.ts
import { post } from '@/utils/request'

export class SpeechToTextAPI {
  async transcribeAudio(
    audioBlob: Blob,
    language: string = 'en'
  ): Promise<STTResult> {
    const formData = new FormData()
    formData.append('audio', audioBlob, 'speech.webm')
    formData.append('language', language)

    return post('/api/v1/speech-to-text/transcribe', formData, {
      headers: {
        'Content-Type': 'multipart/form-data'
      }
    })
  }
}
```

#### Day 6: æ™ºèƒ½é™çº§ç­–ç•¥
**ä»»åŠ¡**:
- [ ] å®ç°å¼•æ“è‡ªåŠ¨é€‰æ‹©ç®—æ³•
- [ ] æ·»åŠ ç½‘ç»œè´¨é‡æ£€æµ‹
- [ ] å®ç°ç¦»çº¿æ¨¡å¼åˆ‡æ¢
- [ ] ä¼˜åŒ–ç”¨æˆ·ä½“éªŒæµç¨‹

**ä»£ç ç¤ºä¾‹**:
```typescript
// utils/adaptiveEngine.ts
export class AdaptiveVoiceRecognition {
  private engines: Map<RecognitionEngineType, RecognitionEngine> = new Map()
  private currentEngine: RecognitionEngine | null = null

  async initialize(): Promise<void> {
    // åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨å¼•æ“
    this.engines.set('webspeech', new WebSpeechEngine())
    this.engines.set('cloud', new CloudSTTEngine())
    this.engines.set('offline', new OfflineEngine())

    // é€‰æ‹©æœ€ä½³å¼•æ“
    this.currentEngine = await this.selectBestEngine()
  }

  private async selectBestEngine(): Promise<RecognitionEngine> {
    const capabilities = await this.detectCapabilities()

    // å†³ç­–é€»è¾‘
    if (capabilities.browser === 'chrome' && capabilities.bandwidth > 1000) {
      return this.engines.get('webspeech')!
    } else if (capabilities.bandwidth > 500) {
      return this.engines.get('cloud')!
    } else {
      return this.engines.get('offline')!
    }
  }

  async switchEngine(engineType: RecognitionEngineType): Promise<void> {
    this.currentEngine = this.engines.get(engineType)!
    // é€šçŸ¥ç”¨æˆ·å¼•æ“å·²åˆ‡æ¢
    this.notifyEngineChange(engineType)
  }
}
```

#### Day 7: ç”¨æˆ·ç•Œé¢ä¼˜åŒ–
**ä»»åŠ¡**:
- [ ] é‡æ–°è®¾è®¡è¯­éŸ³æŒ‰é’®å’ŒçŠ¶æ€æŒ‡ç¤ºå™¨
- [ ] æ·»åŠ å®æ—¶æ³¢å½¢æ˜¾ç¤º
- [ ] å®ç°è¯­éŸ³è´¨é‡æŒ‡ç¤ºå™¨
- [ ] ä¼˜åŒ–é”™è¯¯æç¤ºå’Œå¸®åŠ©ä¿¡æ¯

**ä»£ç ç¤ºä¾‹**:
```vue
<!-- components/VoiceInput.vue -->
<template>
  <div class="voice-input-container">
    <!-- è¯­éŸ³æ³¢å½¢æ˜¾ç¤º -->
    <div class="waveform" v-show="isListening">
      <div
        v-for="(bar, index) in waveformBars"
        :key="index"
        class="waveform-bar"
        :style="{ height: `${bar.height}px` }"
      ></div>
    </div>

    <!-- ä¸»æ§åˆ¶æŒ‰é’® -->
    <button
      class="voice-button"
      :class="{ listening: isListening }"
      @click="toggleListening"
    >
      <el-icon v-if="!isListening"><Microphone /></el-icon>
      <el-icon v-else><SwitchButton /></el-icon>
      {{ buttonText }}
    </button>

    <!-- çŠ¶æ€æŒ‡ç¤ºå™¨ -->
    <div class="status-indicator">
      <el-tag :type="statusTagType">{{ statusText }}</el-tag>
      <el-progress
        v-if="isProcessing"
        :percentage="processingProgress"
        :show-text="false"
      />
    </div>
  </div>
</template>
```

### Phase 3: æµ‹è¯•å’Œä¼˜åŒ– (3å¤©)

#### Day 8: å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
**ä»»åŠ¡**:
- [ ] ç¼–å†™å¼•æ“åˆ‡æ¢å•å…ƒæµ‹è¯•
- [ ] æ·»åŠ æ€§èƒ½ç›‘æ§æµ‹è¯•
- [ ] å®ç°E2Eæµ‹è¯•åœºæ™¯
- [ ] åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•

#### Day 9: å…¼å®¹æ€§æµ‹è¯•
**ä»»åŠ¡**:
- [ ] åœ¨å¤šæµè§ˆå™¨ä¸­æµ‹è¯•
- [ ] éªŒè¯ç§»åŠ¨ç«¯å…¼å®¹æ€§
- [ ] æµ‹è¯•ç½‘ç»œç¯å¢ƒé€‚é…
- [ ] å‹åŠ›æµ‹è¯•å’Œè´Ÿè½½æµ‹è¯•

#### Day 10: ç”¨æˆ·ä½“éªŒä¼˜åŒ–
**ä»»åŠ¡**:
- [ ] æ”¶é›†ç”¨æˆ·åé¦ˆ
- [ ] ä¼˜åŒ–é”™è¯¯æç¤ºæ–‡æ¡ˆ
- [ ] è°ƒæ•´æ€§èƒ½å‚æ•°
- [ ] æ–‡æ¡£å’Œå¸®åŠ©å®Œå–„

---

## ğŸ“Š æˆåŠŸæŒ‡æ ‡

### æŠ€æœ¯æŒ‡æ ‡
| æŒ‡æ ‡ | å½“å‰å€¼ | ç›®æ ‡å€¼ | æµ‹é‡æ–¹æ³• |
|------|--------|--------|----------|
| è¯†åˆ«å‡†ç¡®ç‡ | 70% | 85%+ | å¯¹æ¯”æ ‡å‡†ç­”æ¡ˆ |
| å“åº”å»¶è¿Ÿ | 500ms | 200ms | æµ‹é‡å¼€å§‹åˆ°ç»“æœ |
| æµè§ˆå™¨æ”¯æŒ | 60% | 95% | è¦†ç›–ä¸»æµæµè§ˆå™¨ |
| å´©æºƒç‡ | 3% | <1% | é”™è¯¯æ—¥å¿—ç»Ÿè®¡ |
| å†…å­˜ä½¿ç”¨ | 50MB | 40MB | Performance API |

### ç”¨æˆ·ä½“éªŒæŒ‡æ ‡
| æŒ‡æ ‡ | ç›®æ ‡å€¼ | éªŒè¯æ–¹æ³• |
|------|--------|----------|
| ä¸€é”®æ“ä½œ | 100% | ç”¨æˆ·æ“ä½œæµç¨‹ |
| é”™è¯¯æ¢å¤ | 100% | æ¨¡æ‹Ÿé”™è¯¯åœºæ™¯ |
| å­¦ä¹ æˆæœ¬ | <30ç§’ | ç”¨æˆ·æµ‹è¯• |
| æ»¡æ„åº¦ | >4.5/5 | ç”¨æˆ·åé¦ˆè°ƒæŸ¥ |

---

## ğŸ”§ æŠ€æœ¯é€‰å‹

### ä¾èµ–åº“é€‰æ‹©

#### Web Speech APIå¢å¼º
- **Polyfill**: speech-polyfill.js (Firefoxæ”¯æŒ)
- **ç±»å‹å®šä¹‰**: @types/dom-speech-recognition

#### äº‘ç«¯STTæœåŠ¡
- **OpenAI Whisper**: å‡†ç¡®ç‡é«˜ï¼Œæ”¯æŒå¤šè¯­è¨€
- **Google Cloud Speech**: å®æ—¶æµå¼è¯†åˆ«
- **Azure Speech**: ä¼ä¸šçº§ç¨³å®šæ€§

#### ç¦»çº¿è¯†åˆ«å¼•æ“
- **Vosk WASM**: è½»é‡çº§ï¼Œç¦»çº¿å¯ç”¨
- **æ¨¡å‹å¤§å°**: å‹ç¼©å<50MB
- **æ”¯æŒè¯­è¨€**: è‹±è¯­ã€ä¸­æ–‡ã€æ—¥è¯­ç­‰

#### éŸ³é¢‘å¤„ç†
- **Web Audio API**: æµè§ˆå™¨åŸç”Ÿæ”¯æŒ
- **Wavesurfer.js**: æ³¢å½¢å¯è§†åŒ–
- **RecordRTC**: éŸ³é¢‘å½•åˆ¶å’Œå¤„ç†

### æ€§èƒ½ä¼˜åŒ–å·¥å…·
- **Lighthouse**: æ€§èƒ½å®¡è®¡
- **Web Vitals**: æ ¸å¿ƒæŒ‡æ ‡ç›‘æ§
- **Bundle Analyzer**: åŒ…å¤§å°åˆ†æ

---

## ğŸ’° æˆæœ¬åˆ†æ

### APIè°ƒç”¨æˆæœ¬
- **OpenAI Whisper**: $0.006/åˆ†é’Ÿ (çº¦Â¥0.04/åˆ†é’Ÿ)
- **Google Cloud Speech**: $0.024/åˆ†é’Ÿ (çº¦Â¥0.16/åˆ†é’Ÿ)
- **é¢„ç®—**: 1000æ¬¡/å¤© Ã— 30å¤© = 30000æ¬¡ â‰ˆ Â¥1200/æœˆ

### ä¼˜åŒ–æ”¶ç›Š
- **å‡å°‘å®¢æœå’¨è¯¢**: è¯­éŸ³é—®é¢˜å‡å°‘80%
- **æå‡ç”¨æˆ·ç•™å­˜**: å£è¯­åŠŸèƒ½ä½¿ç”¨ç‡æå‡50%
- **æŠ€æœ¯å€ºåŠ¡å‡å°‘**: é™ä½ç»´æŠ¤æˆæœ¬

---

## ğŸ“š å‚è€ƒèµ„æº

### æ–‡æ¡£å’Œæ•™ç¨‹
- [Web Speech API MDN](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API)
- [OpenAI Whisperæ–‡æ¡£](https://platform.openai.com/docs/guides/speech-to-text)
- [Voskæ–‡æ¡£](https://alphacephei.com/vosk/)

### å¼€æºé¡¹ç›®
- [Vosk WASM](https://github.com/alphacep/vosk-browser)
- [SpeechRecognition Polyfill](https://github.com/TalAter/SpeechRecognition/)
- [RecordRTC](https://recordrtc.org/)

---

## ğŸ“… å®æ–½æ—¶é—´çº¿

| æ—¥æœŸ | ä»»åŠ¡ | è´Ÿè´£äºº | çŠ¶æ€ |
|------|------|--------|------|
| 2026-02-06 | Phase 1: åŸºç¡€ä¼˜åŒ– | å‰ç«¯å›¢é˜Ÿ | å¾…å¼€å§‹ |
| 2026-02-09 | Phase 2: é«˜çº§åŠŸèƒ½ | å…¨æ ˆå›¢é˜Ÿ | å¾…å¼€å§‹ |
| 2026-02-12 | Phase 3: æµ‹è¯•ä¼˜åŒ– | QAå›¢é˜Ÿ | å¾…å¼€å§‹ |
| 2026-02-14 | æœ€ç»ˆéªŒæ”¶ | äº§å“å›¢é˜Ÿ | å¾…å¼€å§‹ |
| 2026-02-15 | MVPå‘å¸ƒ | é¡¹ç›®ç»„ | ç›®æ ‡ |

---

## âš ï¸ é£é™©å’Œç¼“è§£ç­–ç•¥

### æŠ€æœ¯é£é™©
1. **æµè§ˆå™¨å…¼å®¹æ€§é£é™©**
   - **ç¼“è§£**: å®ç°å¤šå±‚é™çº§ç­–ç•¥
   - **å¤‡é€‰**: æä¾›æ–‡æœ¬è¾“å…¥æ›¿ä»£æ–¹æ¡ˆ

2. **APIæˆæœ¬è¶…æ”¯é£é™©**
   - **ç¼“è§£**: å®ç°æ™ºèƒ½ç¼“å­˜å’Œé™é¢æ§åˆ¶
   - **ç›‘æ§**: å®æ—¶APIè°ƒç”¨ç»Ÿè®¡

3. **æ€§èƒ½ä¸‹é™é£é™©**
   - **ç¼“è§£**: æ€§èƒ½åŸºå‡†æµ‹è¯•å’ŒæŒç»­ç›‘æ§
   - **å›æ»š**: å¿«é€Ÿåˆ‡æ¢åˆ°ç¨³å®šç‰ˆæœ¬

### é¡¹ç›®é£é™©
1. **æ—¶é—´ç´§å¼ é£é™©**
   - **ç¼“è§£**: ä¼˜å…ˆçº§æ’åºï¼ŒMVPæ ¸å¿ƒåŠŸèƒ½ä¼˜å…ˆ
   - **å¼¹æ€§**: Phase 3å¯å»¶åè‡³v1.0

2. **æµ‹è¯•è¦†ç›–ä¸è¶³é£é™©**
   - **ç¼“è§£**: è‡ªåŠ¨åŒ–æµ‹è¯•å’ŒCI/CDé›†æˆ
   - **è¡¥å……**: æ‰‹åŠ¨æµ‹è¯•å’Œç”¨æˆ·éªŒæ”¶

---

**è´Ÿè´£äºº**: Claude Code
**å®¡æ ¸**: é¡¹ç›®æŠ€æœ¯å§”å‘˜ä¼š
**ä¸‹æ¬¡æ›´æ–°**: 2026-02-08
