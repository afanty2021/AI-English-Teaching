# è¯­éŸ³è¯†åˆ«ä¼˜åŒ–å¿«é€Ÿå®æ–½æŒ‡å—

> **åˆ›å»ºæ—¶é—´**: 2026-02-05
> **é€‚ç”¨å¯¹è±¡**: å¼€å‘å›¢é˜Ÿ
> **ç›®æ ‡**: 10å¤©å†…å®Œæˆè¯­éŸ³è¯†åˆ«ä¼˜åŒ–

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¬¬ä¸€æ­¥: ç¯å¢ƒå‡†å¤‡

```bash
# 1. ç¡®ä¿Node.jsç‰ˆæœ¬â‰¥18
node --version

# 2. å®‰è£…ä¾èµ–
cd frontend
npm install

# 3. æ£€æŸ¥å½“å‰è¯­éŸ³è¯†åˆ«åŠŸèƒ½
npm run dev
# è®¿é—® http://localhost:5173/student/conversation
```

### ç¬¬äºŒæ­¥: è¯†åˆ«å½“å‰é—®é¢˜

```bash
# è¿è¡ŒLighthouseå®¡è®¡
npx lighthouse http://localhost:5173/student/conversation --view

# æ£€æŸ¥æµè§ˆå™¨å…¼å®¹æ€§
# æ‰“å¼€Chrome, Firefox, Safari, Edgeåˆ†åˆ«æµ‹è¯•
```

---

## ğŸ“ æ ¸å¿ƒä»£ç ç¤ºä¾‹

### 1. æµè§ˆå™¨å…¼å®¹æ€§å¢å¼º

#### 1.1 åˆ›å»ºBrowserCompatibilityç±»

**æ–‡ä»¶**: `frontend/src/utils/browserCompatibility.ts`

```typescript
/**
 * æµè§ˆå™¨å…¼å®¹æ€§æ£€æµ‹
 */
export interface BrowserInfo {
  engine: 'chrome' | 'firefox' | 'safari' | 'edge' | 'unknown'
  version: string
  webSpeechSupported: boolean
  webAudioSupported: boolean
  wasmSupported: boolean
}

export class BrowserCompatibility {
  /**
   * æ£€æµ‹å½“å‰æµè§ˆå™¨ä¿¡æ¯
   */
  static detect(): BrowserInfo {
    const ua = navigator.userAgent

    // æ£€æµ‹å¼•æ“ç±»å‹
    let engine: BrowserInfo['engine'] = 'unknown'
    if (/Chrome/.test(ua) && /Google Inc/.test(navigator.vendor)) {
      engine = 'chrome'
    } else if (/Firefox/.test(ua)) {
      engine = 'firefox'
    } else if (/Safari/.test(ua) && /Apple Computer/.test(navigator.vendor)) {
      engine = 'safari'
    } else if (/Edg/.test(ua)) {
      engine = 'edge'
    }

    // è·å–ç‰ˆæœ¬å·
    const version = this.getVersion(ua)

    return {
      engine,
      version,
      webSpeechSupported: this.checkWebSpeechSupport(),
      webAudioSupported: this.checkWebAudioSupport(),
      wasmSupported: this.checkWasmSupport()
    }
  }

  /**
   * æ£€æŸ¥Web Speech APIæ”¯æŒ
   */
  private static checkWebSpeechSupport(): boolean {
    return !!(
      (window as any).SpeechRecognition ||
      (window as any).webkitSpeechRecognition
    )
  }

  /**
   * æ£€æŸ¥Web Audio APIæ”¯æŒ
   */
  private static checkWebAudioSupport(): boolean {
    return !!(window.AudioContext || (window as any).webkitAudioContext)
  }

  /**
   * æ£€æŸ¥WASMæ”¯æŒ
   */
  private static checkWasmSupport(): boolean {
    return typeof WebAssembly === 'object'
  }

  /**
   * æå–æµè§ˆå™¨ç‰ˆæœ¬
   */
  private static getVersion(ua: string): string {
    const match = ua.match(/(chrome|firefox|safari|edg)\/(\d+)/i)
    return match ? match[2] : 'unknown'
  }

  /**
   * è·å–å…¼å®¹æ€§è¯„åˆ† (0-100)
   */
  static getCompatibilityScore(browser: BrowserInfo): number {
    let score = 0

    if (browser.webSpeechSupported) score += 40
    if (browser.webAudioSupported) score += 30
    if (browser.wasmSupported) score += 20
    if (browser.engine !== 'unknown') score += 10

    return score
  }
}

/**
 * æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦æ”¯æŒè¯­éŸ³è¯†åˆ«
 */
export function isVoiceRecognitionSupported(): boolean {
  const browser = BrowserCompatibility.detect()
  return browser.webSpeechSupported
}

/**
 * è·å–æµè§ˆå™¨ä¿¡æ¯
 */
export function getBrowserInfo(): BrowserInfo {
  return BrowserCompatibility.detect()
}
```

#### 1.2 åœ¨VoiceRecognitionä¸­é›†æˆ

**æ–‡ä»¶**: `frontend/src/utils/voiceRecognition.ts` (ä¿®æ”¹)

```typescript
import { BrowserCompatibility, getBrowserInfo } from './browserCompatibility'

export class VoiceRecognition {
  private recognition: any = null
  private status: VoiceRecognitionStatus = VoiceRecognitionStatus.Idle
  private callbacks: VoiceRecognitionCallbacks = {}
  private config: VoiceRecognitionConfig = {}
  private browserInfo = getBrowserInfo()

  constructor(config: VoiceRecognitionConfig = {}) {
    this.config = {
      language: 'en-US',
      continuous: false,
      interimResults: true,
      maxAlternatives: 1,
      ...config
    }

    // æ£€æŸ¥æµè§ˆå™¨å…¼å®¹æ€§
    const score = BrowserCompatibility.getCompatibilityScore(this.browserInfo)
    if (score < 50) {
      this.setStatus(VoiceRecognitionStatus.Error)
      this.triggerError({
        code: 'browser_incompatible',
        message: `å½“å‰æµè§ˆå™¨å…¼å®¹æ€§é—®é¢˜ (è¯„åˆ†: ${score}/100)ï¼Œå»ºè®®ä½¿ç”¨Chromeæˆ–Firefox`
      })
      return
    }

    this.initRecognition()
  }

  /**
   * åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«ï¼ˆæ”¯æŒé™çº§ï¼‰
   */
  private initRecognition() {
    // ä¼˜å…ˆä½¿ç”¨Web Speech API
    if (this.browserInfo.webSpeechSupported) {
      this.initWebSpeechAPI()
    } else {
      // é™çº§åˆ°å…¶ä»–æ–¹æ¡ˆ
      this.setStatus(VoiceRecognitionStatus.Error)
      this.triggerError({
        code: 'not_supported',
        message: 'æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«ï¼Œå»ºè®®å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬'
      })
    }
  }

  /**
   * åˆå§‹åŒ–Web Speech API
   */
  private initWebSpeechAPI() {
    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition

    if (!SpeechRecognition) {
      this.setStatus(VoiceRecognitionStatus.Error)
      this.triggerError({
        code: 'not_supported',
        message: 'æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½'
      })
      return
    }

    try {
      this.recognition = new SpeechRecognition()
      this.setupRecognition()
    } catch (error) {
      this.setStatus(VoiceRecognitionStatus.Error)
      this.triggerError({
        code: 'init_failed',
        message: 'è¯­éŸ³è¯†åˆ«åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£æƒé™'
      })
    }
  }

  /**
   * é…ç½®è¯­éŸ³è¯†åˆ«äº‹ä»¶
   */
  private setupRecognition() {
    const recognition = this.recognition

    recognition.lang = this.config.language || 'en-US'
    recognition.continuous = this.config.continuous || false
    recognition.interimResults = this.config.interimResults || true
    recognition.maxAlternatives = this.config.maxAlternatives || 1

    // å¼€å§‹è¯†åˆ«
    recognition.onstart = () => {
      this.setStatus(VoiceRecognitionStatus.Listening)
      this.callbacks.onStart?.()
    }

    // è¯†åˆ«ç»“æŸ
    recognition.onend = () => {
      if (this.status === VoiceRecognitionStatus.Listening) {
        this.setStatus(VoiceRecognitionStatus.Idle)
        this.callbacks.onStop?.()
      }
    }

    // è·å–ç»“æœ
    recognition.onresult = (event: any) => {
      const last = event.results.length - 1
      const result = event.results[last]

      const recognitionResult: VoiceRecognitionResult = {
        transcript: result[0].transcript,
        isFinal: result.isFinal,
        confidence: result[0].confidence
      }

      if (result.isFinal) {
        this.callbacks.onResult?.(recognitionResult)
      } else {
        this.callbacks.onInterimResult?.(recognitionResult)
      }
    }

    // é”™è¯¯å¤„ç†
    recognition.onerror = (event: any) => {
      this.handleRecognitionError(event)
    }
  }
}
```

### 2. è¯­éŸ³è´¨é‡å¢å¼º

#### 2.1 åˆ›å»ºAudioEnhancerç±»

**æ–‡ä»¶**: `frontend/src/utils/audioEnhancer.ts`

```typescript
/**
 * éŸ³é¢‘å¢å¼ºå·¥å…·
 */
export class AudioEnhancer {
  private audioContext: AudioContext | null = null
  private analyser: AnalyserNode | null = null
  private gainNode: GainNode | null = null

  constructor() {
    this.initAudioContext()
  }

  /**
   * åˆå§‹åŒ–éŸ³é¢‘ä¸Šä¸‹æ–‡
   */
  private initAudioContext() {
    try {
      this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
      this.analyser = this.audioContext.createAnalyser()
      this.gainNode = this.audioContext.createGain()

      this.analyser.fftSize = 512
      this.analyser.smoothingTimeConstant = 0.8
    } catch (error) {
      console.error('AudioContextåˆå§‹åŒ–å¤±è´¥:', error)
    }
  }

  /**
   * å¢å¼ºéŸ³é¢‘æµ
   */
  async enhanceStream(stream: MediaStream): Promise<MediaStream> {
    if (!this.audioContext || !this.analyser || !this.gainNode) {
      console.warn('AudioContextæœªåˆå§‹åŒ–ï¼Œè¿”å›åŸå§‹æµ')
      return stream
    }

    try {
      // åˆ›å»ºå™ªéŸ³æŠ‘åˆ¶å™¨
      const noiseSuppressor = this.audioContext.createDynamicsCompressor()
      noiseSuppressor.threshold.setValueAtTime(-50, this.audioContext.currentTime)
      noiseSuppressor.knee.setValueAtTime(40, this.audioContext.currentTime)
      noiseSuppressor.ratio.setValueAtTime(12, this.audioContext.currentTime)
      noiseSuppressor.attack.setValueAtTime(0, this.audioContext.currentTime)
      noiseSuppressor.release.setValueAtTime(0.25, this.audioContext.currentTime)

      // åˆ›å»ºé«˜é€šæ»¤æ³¢å™¨ï¼ˆå»é™¤ä½é¢‘å™ªéŸ³ï¼‰
      const highpassFilter = this.audioContext.createBiquadFilter()
      highpassFilter.type = 'highpass'
      highpassFilter.frequency.setValueAtTime(80, this.audioContext.currentTime)

      // åˆ›å»ºä½é€šæ»¤æ³¢å™¨ï¼ˆå»é™¤é«˜é¢‘å™ªéŸ³ï¼‰
      const lowpassFilter = this.audioContext.createBiquadFilter()
      lowpassFilter.type = 'lowpass'
      lowpassFilter.frequency.setValueAtTime(8000, this.audioContext.currentTime)

      // è¿æ¥éŸ³é¢‘å¤„ç†å›¾
      const source = this.audioContext.createMediaStreamSource(stream)
      source.connect(highpassFilter)
      highpassFilter.connect(lowpassFilter)
      lowpassFilter.connect(noiseSuppressor)
      noiseSuppressor.connect(this.analyser)
      this.analyser.connect(this.gainNode)
      this.gainNode.connect(this.audioContext.destination)

      console.log('éŸ³é¢‘æµå¢å¼ºå®Œæˆ')
      return stream
    } catch (error) {
      console.error('éŸ³é¢‘æµå¢å¼ºå¤±è´¥:', error)
      return stream
    }
  }

  /**
   * æ£€æµ‹è¯­éŸ³æ´»åŠ¨
   */
  async detectVoiceActivity(stream: MediaStream): Promise<boolean> {
    if (!this.audioContext || !this.analyser) {
      return false
    }

    return new Promise((resolve) => {
      try {
        const source = this.audioContext.createMediaStreamSource(stream)
        source.connect(this.analyser)

        const dataArray = new Uint8Array(this.analyser.frequencyBinCount)

        const check = () => {
          this.analyser!.getByteFrequencyData(dataArray)
          const average = dataArray.reduce((a, b) => a + b) / dataArray.length

          // åŠ¨æ€é˜ˆå€¼ (å¯æ ¹æ®ç¯å¢ƒè°ƒæ•´)
          const threshold = 30
          resolve(average > threshold)
        }

        check()
      } catch (error) {
        console.error('è¯­éŸ³æ´»åŠ¨æ£€æµ‹å¤±è´¥:', error)
        resolve(false)
      }
    })
  }

  /**
   * è·å–å½“å‰éŸ³é‡çº§åˆ« (0-100)
   */
  getVolumeLevel(stream: MediaStream): number {
    if (!this.audioContext || !this.analyser) {
      return 0
    }

    try {
      const source = this.audioContext.createMediaStreamSource(stream)
      source.connect(this.analyser)

      const dataArray = new Uint8Array(this.analyser.frequencyBinCount)
      this.analyser.getByteFrequencyData(dataArray)

      const sum = dataArray.reduce((a, b) => a + b)
      const average = sum / dataArray.length

      // è½¬æ¢ä¸ºç™¾åˆ†æ¯”
      return Math.min(100, Math.round((average / 255) * 100))
    } catch (error) {
      console.error('éŸ³é‡æ£€æµ‹å¤±è´¥:', error)
      return 0
    }
  }

  /**
   * æ£€æŸ¥éº¦å…‹é£æƒé™
   */
  async checkMicrophonePermission(): Promise<boolean> {
    try {
      const result = await navigator.permissions.query({ name: 'microphone' as PermissionName })
      return result.state === 'granted'
    } catch (error) {
      console.error('æƒé™æ£€æŸ¥å¤±è´¥:', error)
      return false
    }
  }

  /**
   * è¯·æ±‚éº¦å…‹é£æƒé™
   */
  async requestMicrophonePermission(): Promise<boolean> {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      stream.getTracks().forEach(track => track.stop()) // ç«‹å³åœæ­¢
      return true
    } catch (error) {
      console.error('éº¦å…‹é£æƒé™è¯·æ±‚å¤±è´¥:', error)
      return false
    }
  }

  /**
   * é”€æ¯èµ„æº
   */
  destroy() {
    if (this.audioContext) {
      this.audioContext.close()
      this.audioContext = null
    }
    this.analyser = null
    this.gainNode = null
  }
}

/**
 * åˆ›å»ºéŸ³é¢‘å¢å¼ºå™¨å®ä¾‹
 */
export function createAudioEnhancer(): AudioEnhancer {
  return new AudioEnhancer()
}
```

### 3. æ€§èƒ½ç›‘æ§

#### 3.1 åˆ›å»ºPerformanceMonitorç±»

**æ–‡ä»¶**: `frontend/src/utils/performanceMonitor.ts`

```typescript
/**
 * è¯­éŸ³è¯†åˆ«æ€§èƒ½ç›‘æ§
 */
export interface RecognitionMetrics {
  accuracy: number
  latency: number
  errorRate: number
  usageCount: number
  lastUsed: Date
}

export class PerformanceMonitor {
  private metrics: RecognitionMetrics = {
    accuracy: 0,
    latency: 0,
    errorRate: 0,
    usageCount: 0,
    lastUsed: new Date()
  }

  private latencyHistory: number[] = []
  private accuracyHistory: number[] = []
  private readonly maxHistorySize = 100

  /**
   * è®°å½•ä¸€æ¬¡è¯†åˆ«
   */
  trackRecognition(
    startTime: number,
    endTime: number,
    isSuccessful: boolean,
    confidence: number = 0
  ): void {
    const latency = endTime - startTime
    const now = new Date()

    // æ›´æ–°ä½¿ç”¨æ¬¡æ•°
    this.metrics.usageCount++

    // æ›´æ–°å»¶è¿Ÿ
    this.metrics.latency = latency
    this.latencyHistory.push(latency)
    if (this.latencyHistory.length > this.maxHistorySize) {
      this.latencyHistory.shift()
    }

    // æ›´æ–°å‡†ç¡®ç‡
    if (isSuccessful) {
      const newAccuracy = (this.metrics.accuracy * (this.metrics.usageCount - 1) + confidence) / this.metrics.usageCount
      this.metrics.accuracy = newAccuracy
      this.accuracyHistory.push(confidence)
      if (this.accuracyHistory.length > this.maxHistorySize) {
        this.accuracyHistory.shift()
      }
    } else {
      const newErrorRate = (this.metrics.errorRate * (this.metrics.usageCount - 1) + 1) / this.metrics.usageCount
      this.metrics.errorRate = newErrorRate
    }

    // æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
    this.metrics.lastUsed = now
  }

  /**
   * è·å–æ€§èƒ½æŒ‡æ ‡
   */
  getMetrics(): RecognitionMetrics {
    return { ...this.metrics }
  }

  /**
   * è·å–å»¶è¿Ÿç»Ÿè®¡
   */
  getLatencyStats(): { min: number; max: number; avg: number; p95: number } {
    if (this.latencyHistory.length === 0) {
      return { min: 0, max: 0, avg: 0, p95: 0 }
    }

    const sorted = [...this.latencyHistory].sort((a, b) => a - b)
    const sum = sorted.reduce((a, b) => a + b, 0)

    return {
      min: sorted[0],
      max: sorted[sorted.length - 1],
      avg: Math.round(sum / sorted.length),
      p95: sorted[Math.floor(sorted.length * 0.95)]
    }
  }

  /**
   * è·å–å‡†ç¡®ç‡ç»Ÿè®¡
   */
  getAccuracyStats(): { min: number; max: number; avg: number } {
    if (this.accuracyHistory.length === 0) {
      return { min: 0, max: 0, avg: 0 }
    }

    const sorted = [...this.accuracyHistory].sort((a, b) => a - b)
    const sum = sorted.reduce((a, b) => a + b, 0)

    return {
      min: sorted[0],
      max: sorted[sorted.length - 1],
      avg: Math.round((sum / sorted.length) * 100) / 100
    }
  }

  /**
   * é‡ç½®ç»Ÿè®¡
   */
  reset(): void {
    this.metrics = {
      accuracy: 0,
      latency: 0,
      errorRate: 0,
      usageCount: 0,
      lastUsed: new Date()
    }
    this.latencyHistory = []
    this.accuracyHistory = []
  }

  /**
   * å¯¼å‡ºæ€§èƒ½æŠ¥å‘Š
   */
  exportReport(): string {
    const latencyStats = this.getLatencyStats()
    const accuracyStats = this.getAccuracyStats()

    return `
è¯­éŸ³è¯†åˆ«æ€§èƒ½æŠ¥å‘Š
================
ä½¿ç”¨æ¬¡æ•°: ${this.metrics.usageCount}
æœ€åä½¿ç”¨: ${this.metrics.lastUsed.toLocaleString()}

å»¶è¿Ÿç»Ÿè®¡:
  å¹³å‡: ${latencyStats.avg}ms
  æœ€å°: ${latencyStats.min}ms
  æœ€å¤§: ${latencyStats.max}ms
  95%åˆ†ä½: ${latencyStats.p95}ms

å‡†ç¡®ç‡ç»Ÿè®¡:
  å¹³å‡: ${accuracyStats.avg}%
  æœ€ä½: ${accuracyStats.min}%
  æœ€é«˜: ${accuracyStats.max}%

é”™è¯¯ç‡: ${Math.round(this.metrics.errorRate * 100)}%
`
  }
}

/**
 * åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨å®ä¾‹
 */
export function createPerformanceMonitor(): PerformanceMonitor {
  return new PerformanceMonitor()
}
```

### 4. è‡ªé€‚åº”è¯­éŸ³è¯†åˆ«

#### 4.1 åˆ›å»ºAdaptiveVoiceRecognitionç±»

**æ–‡ä»¶**: `frontend/src/utils/adaptiveVoiceRecognition.ts`

```typescript
import { VoiceRecognition } from './voiceRecognition'
import { BrowserCompatibility } from './browserCompatibility'
import { AudioEnhancer } from './audioEnhancer'
import { PerformanceMonitor } from './performanceMonitor'

export type RecognitionEngineType = 'webspeech' | 'cloud' | 'offline'

export interface EngineInfo {
  type: RecognitionEngineType
  name: string
  accuracy: number
  latency: number
  cost: number
  offline: boolean
}

/**
 * è‡ªé€‚åº”è¯­éŸ³è¯†åˆ«å¼•æ“
 */
export class AdaptiveVoiceRecognition {
  private engines = new Map<RecognitionEngineType, any>()
  private currentEngine: any = null
  private currentEngineType: RecognitionEngineType | null = null
  private browserInfo = BrowserCompatibility.detect()
  private audioEnhancer = new AudioEnhancer()
  private performanceMonitor = new PerformanceMonitor()

  constructor() {
    this.initializeEngines()
  }

  /**
   * åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨å¼•æ“
   */
  private async initializeEngines(): Promise<void> {
    // 1. Web Speech APIå¼•æ“
    if (this.browserInfo.webSpeechSupported) {
      this.engines.set('webspeech', {
        type: 'webspeech' as RecognitionEngineType,
        init: () => new VoiceRecognition(),
        name: 'Web Speech API'
      })
    }

    // 2. äº‘ç«¯STTå¼•æ“ (å¾…å®ç°)
    this.engines.set('cloud', {
      type: 'cloud' as RecognitionEngineType,
      init: () => null, // å¾…å®ç°
      name: 'Cloud STT'
    })

    // 3. ç¦»çº¿å¼•æ“ (å¾…å®ç°)
    this.engines.set('offline', {
      type: 'offline' as RecognitionEngineType,
      init: () => null, // å¾…å®ç°
      name: 'Offline STT'
    })
  }

  /**
   * é€‰æ‹©æœ€ä½³å¼•æ“
   */
  async selectBestEngine(): Promise<RecognitionEngineType> {
    // è¯„ä¼°æ¯ä¸ªå¼•æ“çš„å¯ç”¨æ€§
    const engineScores = new Map<RecognitionEngineType, number>()

    // Web Speech APIè¯„ä¼°
    if (this.engines.has('webspeech')) {
      let score = 0
      if (this.browserInfo.engine === 'chrome' || this.browserInfo.engine === 'edge') {
        score += 50 // Chrome/Edgeæ”¯æŒæœ€å¥½
      } else if (this.browserInfo.engine === 'firefox') {
        score += 30 // Firefoxéœ€è¦polyfill
      } else if (this.browserInfo.engine === 'safari') {
        score += 20 // Safariæ”¯æŒæœ‰é™
      }

      // ç½‘ç»œè´¨é‡è¯„ä¼°
      const networkQuality = await this.measureNetworkQuality()
      if (networkQuality < 500) {
        score += 30 // æ…¢é€Ÿç½‘ç»œä¼˜å…ˆæœ¬åœ°
      }

      engineScores.set('webspeech', score)
    }

    // äº‘ç«¯STTè¯„ä¼°
    const networkQuality = await this.measureNetworkQuality()
    if (networkQuality > 1000) {
      engineScores.set('cloud', 40)
    }

    // ç¦»çº¿å¼•æ“è¯„ä¼° (æ€»æ˜¯å¯ç”¨)
    engineScores.set('offline', 10)

    // é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¼•æ“
    let bestEngine = 'webspeech' as RecognitionEngineType
    let maxScore = 0

    for (const [engine, score] of engineScores.entries()) {
      if (score > maxScore) {
        maxScore = score
        bestEngine = engine
      }
    }

    console.log(`é€‰æ‹©è¯­éŸ³è¯†åˆ«å¼•æ“: ${bestEngine}, å¾—åˆ†: ${maxScore}`)
    return bestEngine
  }

  /**
   * æµ‹é‡ç½‘ç»œè´¨é‡
   */
  private async measureNetworkQuality(): Promise<number> {
    try {
      const start = performance.now()
      await fetch('/api/health', { method: 'HEAD', cache: 'no-store' })
      const latency = performance.now() - start

      // ç®€å•çš„å¸¦å®½ä¼°ç®—
      const connection = (navigator as any).connection
      if (connection && connection.downlink) {
        return connection.downlink * 1000 // è½¬æ¢ä¸ºkbps
      }

      return latency < 200 ? 2000 : 500
    } catch (error) {
      console.warn('ç½‘ç»œè´¨é‡æ£€æµ‹å¤±è´¥:', error)
      return 100
    }
  }

  /**
   * åˆ‡æ¢å¼•æ“
   */
  async switchEngine(engineType: RecognitionEngineType): Promise<void> {
    if (!this.engines.has(engineType)) {
      throw new Error(`å¼•æ“ ${engineType} ä¸å¯ç”¨`)
    }

    console.log(`åˆ‡æ¢è¯­éŸ³è¯†åˆ«å¼•æ“: ${this.currentEngineType} -> ${engineType}`)

    // é”€æ¯å½“å‰å¼•æ“
    if (this.currentEngine && this.currentEngine.destroy) {
      this.currentEngine.destroy()
    }

    // åˆå§‹åŒ–æ–°å¼•æ“
    const engineInfo = this.engines.get(engineType)!
    this.currentEngine = engineInfo.init()
    this.currentEngineType = engineType

    // é€šçŸ¥å¼•æ“åˆ‡æ¢
    this.notifyEngineChange(engineType)
  }

  /**
   * å¼€å§‹è¯†åˆ«
   */
  async start(): Promise<void> {
    if (!this.currentEngine) {
      // è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¼•æ“
      const bestEngine = await this.selectBestEngine()
      await this.switchEngine(bestEngine)
    }

    if (this.currentEngine && this.currentEngine.start) {
      const startTime = performance.now()

      try {
        await this.currentEngine.start()
        this.performanceMonitor.trackRecognition(startTime, performance.now(), true, 1)
      } catch (error) {
        this.performanceMonitor.trackRecognition(startTime, performance.now(), false, 0)
        throw error
      }
    }
  }

  /**
   * åœæ­¢è¯†åˆ«
   */
  async stop(): Promise<void> {
    if (this.currentEngine && this.currentEngine.stop) {
      await this.currentEngine.stop()
    }
  }

  /**
   * æ³¨å†Œå›è°ƒ
   */
  on(callbacks: any): void {
    if (this.currentEngine && this.currentEngine.on) {
      this.currentEngine.on(callbacks)
    }
  }

  /**
   * è·å–å½“å‰å¼•æ“ä¿¡æ¯
   */
  getCurrentEngine(): EngineInfo | null {
    if (!this.currentEngineType) {
      return null
    }

    const engine = this.engines.get(this.currentEngineType)!
    return {
      type: engine.type,
      name: engine.name,
      accuracy: 85, // å¾…å®ç°
      latency: 200, // å¾…å®ç°
      cost: engine.type === 'cloud' ? 0.01 : 0,
      offline: engine.type === 'webspeech' || engine.type === 'offline'
    }
  }

  /**
   * é€šçŸ¥å¼•æ“åˆ‡æ¢
   */
  private notifyEngineChange(engineType: RecognitionEngineType): void {
    console.log(`è¯­éŸ³è¯†åˆ«å¼•æ“å·²åˆ‡æ¢ä¸º: ${engineType}`)
    // å¯ä»¥åœ¨è¿™é‡Œè§¦å‘UIæ›´æ–°
  }

  /**
   * è·å–æ€§èƒ½æŒ‡æ ‡
   */
  getMetrics(): any {
    return this.performanceMonitor.getMetrics()
  }

  /**
   * é”€æ¯èµ„æº
   */
  destroy(): void {
    if (this.currentEngine && this.currentEngine.destroy) {
      this.currentEngine.destroy()
    }
    this.audioEnhancer.destroy()
  }
}

/**
 * åˆ›å»ºè‡ªé€‚åº”è¯­éŸ³è¯†åˆ«å®ä¾‹
 */
export function createAdaptiveVoiceRecognition(): AdaptiveVoiceRecognition {
  return new AdaptiveVoiceRecognition()
}
```

---

## ğŸ¨ UIç»„ä»¶ç¤ºä¾‹

### è¯­éŸ³è¾“å…¥ç»„ä»¶

**æ–‡ä»¶**: `frontend/src/components/VoiceInput.vue`

```vue
<template>
  <div class="voice-input-container">
    <!-- è¯­éŸ³æ³¢å½¢æ˜¾ç¤º -->
    <div class="waveform" v-show="isListening">
      <div
        v-for="(bar, index) in waveformBars"
        :key="index"
        class="waveform-bar"
        :style="{ height: `${bar.height}px` }"
      ></div>
    </div>

    <!-- ä¸»æ§åˆ¶æŒ‰é’® -->
    <button
      class="voice-button"
      :class="{ listening: isListening }"
      @click="toggleListening"
      :disabled="isProcessing"
    >
      <el-icon v-if="!isListening"><Microphone /></el-icon>
      <el-icon v-else><SwitchButton /></el-icon>
      {{ buttonText }}
    </button>

    <!-- çŠ¶æ€æŒ‡ç¤ºå™¨ -->
    <div class="status-indicator">
      <el-tag :type="statusTagType">{{ statusText }}</el-tag>
      <div v-if="isListening" class="volume-indicator">
        <div class="volume-bar" :style="{ width: `${volumeLevel}%` }"></div>
      </div>
    </div>

    <!-- é”™è¯¯æç¤º -->
    <el-alert
      v-if="error"
      :title="error.message"
      type="error"
      :closable="false"
      show-icon
    />
  </div>
</template>

<script setup lang="ts">
import { ref, computed, onMounted, onUnmounted } from 'vue'
import { ElMessage } from 'element-plus'
import { Microphone, SwitchButton } from '@element-plus/icons-vue'
import { AdaptiveVoiceRecognition } from '@/utils/adaptiveVoiceRecognition'

const voiceRecognition = ref<AdaptiveVoiceRecognition | null>(null)
const isListening = ref(false)
const isProcessing = ref(false)
const error = ref<{ message: string } | null>(null)
const volumeLevel = ref(0)
const waveformBars = ref<{ height: number }[]>(new Array(20).fill(0).map(() => ({ height: 10 })))

// è®¡ç®—å±æ€§
const buttonText = computed(() => {
  if (isProcessing.value) return 'å¤„ç†ä¸­...'
  if (isListening.value) return 'ç‚¹å‡»åœæ­¢'
  return 'ç‚¹å‡»è¯´è¯'
})

const statusText = computed(() => {
  if (error.value) return 'é”™è¯¯'
  if (isProcessing.value) return 'å¤„ç†ä¸­'
  if (isListening.value) return 'æ­£åœ¨å¬...'
  return 'å‡†å¤‡å°±ç»ª'
})

const statusTagType = computed(() => {
  if (error.value) return 'danger'
  if (isProcessing.value) return 'warning'
  if (isListening.value) return 'success'
  return 'info'
})

// æ–¹æ³•
const toggleListening = async () => {
  try {
    if (isListening.value) {
      await voiceRecognition.value?.stop()
      isListening.value = false
    } else {
      error.value = null
      await voiceRecognition.value?.start()
      isListening.value = true
    }
  } catch (err: any) {
    error.value = { message: err.message }
    ElMessage.error(err.message)
  }
}

const updateVolumeLevel = () => {
  // æ¨¡æ‹ŸéŸ³é‡å˜åŒ– (å®é™…åº”ä»AudioEnhancerè·å–)
  volumeLevel.value = Math.random() * 100
}

onMounted(() => {
  voiceRecognition.value = new AdaptiveVoiceRecognition()

  voiceRecognition.value.on({
    onStart: () => {
      isListening.value = true
      isProcessing.value = false
    },
    onStop: () => {
      isListening.value = false
    },
    onResult: (result: any) => {
      isProcessing.value = false
      console.log('è¯†åˆ«ç»“æœ:', result.transcript)
      // å¤„ç†è¯†åˆ«ç»“æœ
    },
    onError: (err: any) => {
      error.value = err
      isListening.value = false
      isProcessing.value = false
    }
  })

  // æ¨¡æ‹ŸéŸ³é‡æ›´æ–°
  const volumeInterval = setInterval(updateVolumeLevel, 100)
  onUnmounted(() => clearInterval(volumeInterval))
})

onUnmounted(() => {
  voiceRecognition.value?.destroy()
})
</script>

<style scoped>
.voice-input-container {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 20px;
  padding: 20px;
}

.voice-button {
  width: 80px;
  height: 80px;
  border-radius: 50%;
  border: none;
  background: var(--el-color-primary);
  color: white;
  font-size: 24px;
  cursor: pointer;
  transition: all 0.3s;
}

.voice-button:hover {
  transform: scale(1.1);
}

.voice-button.listening {
  background: var(--el-color-success);
  animation: pulse 1.5s infinite;
}

@keyframes pulse {
  0%, 100% { transform: scale(1); }
  50% { transform: scale(1.05); }
}

.waveform {
  display: flex;
  gap: 4px;
  height: 40px;
  align-items: center;
}

.waveform-bar {
  width: 4px;
  background: var(--el-color-primary);
  border-radius: 2px;
  animation: waveform 1s ease-in-out infinite;
}

.waveform-bar:nth-child(2n) {
  animation-delay: 0.1s;
}

@keyframes waveform {
  0%, 100% { height: 10px; }
  50% { height: 30px; }
}

.volume-indicator {
  width: 100px;
  height: 4px;
  background: #eee;
  border-radius: 2px;
  overflow: hidden;
  margin-top: 8px;
}

.volume-bar {
  height: 100%;
  background: var(--el-color-primary);
  transition: width 0.1s;
}
</style>
```

---

## âœ… æµ‹è¯•æ£€æŸ¥æ¸…å•

### åŠŸèƒ½æµ‹è¯•
- [ ] Chromeæµè§ˆå™¨è¯­éŸ³è¯†åˆ«æ­£å¸¸
- [ ] Firefoxæµè§ˆå™¨é™çº§å¤„ç†æ­£å¸¸
- [ ] Safariæµè§ˆå™¨æç¤ºå‹å¥½
- [ ] é”™è¯¯æç¤ºæ¸…æ™°å‡†ç¡®
- [ ] æƒé™è¯·æ±‚æµç¨‹é¡ºç•…

### æ€§èƒ½æµ‹è¯•
- [ ] è¯†åˆ«å»¶è¿Ÿ<200ms
- [ ] å‡†ç¡®ç‡>85%
- [ ] å†…å­˜ä½¿ç”¨<50MB
- [ ] CPUå ç”¨<20%

### å…¼å®¹æ€§æµ‹è¯•
- [ ] Windows Chrome
- [ ] macOS Safari
- [ ] iOS Safari
- [ ] Android Chrome
- [ ] å„ç§ç½‘ç»œç¯å¢ƒ

---

## ğŸš¨ å¸¸è§é—®é¢˜è§£å†³

### 1. éº¦å…‹é£æƒé™è¢«æ‹’ç»
```typescript
// æ£€æŸ¥æƒé™
const permission = await navigator.permissions.query({ name: 'microphone' })

if (permission.state === 'denied') {
  ElMessageBox.alert(
    'éœ€è¦éº¦å…‹é£æƒé™æ‰èƒ½ä½¿ç”¨è¯­éŸ³è¯†åˆ«',
    'æƒé™è¢«æ‹’ç»',
    {
      confirmButtonText: 'å‰å¾€è®¾ç½®',
      cancelButtonText: 'å–æ¶ˆ',
      type: 'warning'
    }
  ).then(() => {
    // å¼•å¯¼ç”¨æˆ·æ‰“å¼€è®¾ç½®
    window.open('chrome://settings/content/microphone')
  })
}
```

### 2. Web Speech APIåˆå§‹åŒ–å¤±è´¥
```typescript
// æ·»åŠ é‡è¯•æœºåˆ¶
let retryCount = 0
const maxRetries = 3

const initWithRetry = async () => {
  try {
    this.initRecognition()
  } catch (error) {
    retryCount++
    if (retryCount < maxRetries) {
      setTimeout(initWithRetry, 1000 * retryCount)
    } else {
      this.triggerError({
        code: 'init_failed',
        message: 'è¯­éŸ³è¯†åˆ«åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•'
      })
    }
  }
}
```

---

**æœ€åæ›´æ–°**: 2026-02-05
**ç‰ˆæœ¬**: v1.0
