# è¯­éŸ³è¯†åˆ«é›†æˆä¼˜åŒ–å®æ–½è®¡åˆ’

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** ä¼˜åŒ–è¯­éŸ³è¯†åˆ«é›†æˆï¼Œæå‡è¯†åˆ«å‡†ç¡®ç‡ã€ç”¨æˆ·ä½“éªŒå’Œæµè§ˆå™¨å…¼å®¹æ€§

**Architecture:** æ··åˆè¯­éŸ³è¯†åˆ«æ¶æ„ - å‰ç«¯ Web Speech API + åç«¯ Whisper APIï¼Œæ™ºèƒ½é™çº§ç­–ç•¥

**Tech Stack:** Vue3, TypeScript, Web Speech API, OpenAI Whisper, Web Audio API, Qdrant

---

## ğŸ“‹ è®¡åˆ’æ¦‚è§ˆ

### å½“å‰çŠ¶æ€
- âœ… åŸºç¡€è¯­éŸ³è¯†åˆ«å·²å®ç°
- âœ… STT/TTS å·¥å…·ç±»å®Œæ•´
- âœ… å¯¹è¯æµç¨‹é›†æˆæµ‹è¯•é€šè¿‡ (23/23)
- âš ï¸ Safari å…¼å®¹æ€§å¾…å¤„ç†
- âš ï¸ ç½‘ç»œé™çº§ç­–ç•¥å¾…å®Œå–„
- âš ï¸ è¯†åˆ«å‡†ç¡®ç‡ç›‘æ§å¾…åŠ å¼º

### ä¼˜åŒ–ç›®æ ‡
1. **å…¼å®¹æ€§ä¼˜åŒ–**: å®Œå–„ Safari/Firefox æ”¯æŒï¼Œæ·»åŠ é™çº§ç­–ç•¥
2. **æ€§èƒ½ä¼˜åŒ–**: å®ç°éŸ³é¢‘ç¼“å†²ã€LRU ç¼“å­˜ã€å»¶è¿Ÿåˆ†è§£ä¼˜åŒ–
3. **ç”¨æˆ·ä½“éªŒ**: æ·»åŠ å®æ—¶åé¦ˆã€ç½®ä¿¡åº¦æ˜¾ç¤ºã€å¯è§†åŒ–å¢å¼º
4. **è´¨é‡æå‡**: éŸ³é¢‘é¢„å¤„ç†ã€é™å™ªå¢å¼ºã€å‡†ç¡®ç‡ç›‘æ§

### é¢„è®¡å·¥ä½œé‡
- æ€»è®¡ 15 ä¸ªä»»åŠ¡
- é¢„è®¡ 2-3 å¤©å®Œæˆ
- æ¶‰åŠå‰ç«¯ 8 ä¸ªæ–‡ä»¶ã€åç«¯ 4 ä¸ªæ–‡ä»¶

---

## Phase 1: æµè§ˆå™¨å…¼å®¹æ€§ä¼˜åŒ– (ä¼˜å…ˆçº§: ğŸ”´ é«˜)

### Task 1: åˆ›å»ºæµè§ˆå™¨å…¼å®¹æ€§é™çº§ç­–ç•¥

**Files:**
- Modify: `frontend/src/utils/browserCompatibility.ts`
- Create: `frontend/src/utils/voiceRecognitionFallback.ts`
- Test: `frontend/tests/unit/voiceRecognitionFallback.spec.ts`

**Step 1: åˆ†æå½“å‰æµè§ˆå™¨å…¼å®¹æ€§æ£€æµ‹ä»£ç **

```typescript
// æŸ¥çœ‹ browserCompatibility.ts ä¸­çš„æ£€æµ‹é€»è¾‘
// å…³æ³¨ Safari å’Œ Firefox çš„å…¼å®¹æ€§é—®é¢˜
```

Run: `npm run type-check`
Expected: ç±»å‹æ£€æŸ¥é€šè¿‡

**Step 2: è®¾è®¡é™çº§ç­–ç•¥æ¥å£**

```typescript
// frontend/src/utils/voiceRecognitionFallback.ts

export interface VoiceRecognitionFallback {
  canUseWebSpeechAPI(): boolean
  canUseCloudSTT(): boolean
  getRecommendedStrategy(): RecognitionStrategy
  createRecognition(options: RecognitionConfig): VoiceRecognitionBase
}

export enum RecognitionStrategy {
  WebSpeechAPI = 'web_speech_api',        // Chrome/Edge æœ€ä½³
  CloudSTT = 'cloud_stt',                  // åç«¯ Whisper
  Hybrid = 'hybrid',                      // å‰ç«¯+åç«¯æ··åˆ
  Disabled = 'disabled'                   // å®Œå…¨ä¸æ”¯æŒ
}
```

**Step 3: å®ç°é™çº§ç­–ç•¥å†³ç­–å™¨**

```typescript
export class VoiceRecognitionFallback implements VoiceRecognitionFallback {
  constructor(private browserCompat: BrowserCompatibility) {}

  canUseWebSpeechAPI(): boolean {
    return this.browserCompat.webSpeechSupported &&
           this.browserCompat.engine === 'chrome' ||
           this.browserCompat.engine === 'edge'
  }

  canUseCloudSTT(): boolean {
    // æ£€æŸ¥æ˜¯å¦åœ¨ç½‘ç»œç¯å¢ƒä¸”æœ‰ API key
    return !!import.meta.env.VITE_OPENAI_API_KEY
  }

  getRecommendedStrategy(): RecognitionStrategy {
    // 1. ä¼˜å…ˆä½¿ç”¨æµè§ˆå™¨å†…ç½® APIï¼ˆæœ€å¿«ã€æ— æˆæœ¬ï¼‰
    if (this.canUseWebSpeechAPI()) {
      return RecognitionStrategy.WebSpeechAPI
    }

    // 2. é™çº§åˆ°äº‘ç«¯ STT
    if (this.canUseCloudSTT()) {
      return RecognitionStrategy.CloudSTT
    }

    // 3. å®Œå…¨ä¸æ”¯æŒï¼Œæ˜¾ç¤ºæç¤º
    return RecognitionStrategy.Disabled
  }

  createRecognition(config: RecognitionConfig): VoiceRecognitionBase {
    const strategy = this.getRecommendedStrategy()

    switch (strategy) {
      case RecognitionStrategy.WebSpeechAPI:
        return new WebSpeechRecognitionAdapter(config)

      case RecognitionStrategy.CloudSTT:
        return new CloudSTTAdapter(config)

      case RecognitionStrategy.Disabled:
        throw new Error('å½“å‰æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«ï¼Œè¯·ä½¿ç”¨ Chrome æˆ– Edge æµè§ˆå™¨')

      default:
        throw new Error(`æœªçŸ¥çš„è¯†åˆ«ç­–ç•¥: ${strategy}`)
    }
  }
}
```

**Step 4: ç¼–å†™é™çº§ç­–ç•¥æµ‹è¯•**

```typescript
// frontend/tests/unit/voiceRecognitionFallback.spec.ts

import { describe, it, expect, vi } from 'vitest'
import { BrowserCompatibility } from '@/utils/browserCompatibility'
import { VoiceRecognitionFallback, RecognitionStrategy } from '@/utils/voiceRecognitionFallback'

describe('VoiceRecognitionFallback', () => {
  it('should recommend WebSpeechAPI for Chrome', () => {
    const compat = new BrowserCompatibility()
    vi.spyOn(compat, 'detect').mockReturnValue({
      webSpeechSupported: true,
      engine: 'chrome'
    })

    const fallback = new VoiceRecognitionFallback(compat)
    expect(fallback.getRecommendedStrategy()).toBe(RecognitionStrategy.WebSpeechAPI)
  })

  it('should recommend CloudSTT for Safari', () => {
    const compat = new BrowserCompatibility()
    vi.spyOn(compat, 'detect').mockReturnValue({
      webSpeechSupported: false,
      engine: 'safari'
    })

    const fallback = new VoiceRecognitionFallback(compat)
    expect(fallback.getRecommendedStrategy()).toBe(RecognitionStrategy.CloudSTT)
  })

  it('should throw error for unsupported browser', () => {
    const compat = new BrowserCompatibility()
    vi.spyOn(compat, 'detect').mockReturnValue({
      webSpeechSupported: false,
      engine: 'unknown'
    })

    const fallback = new VoiceRecognitionFallback(compat)
    expect(() => fallback.getRecommendedStrategy()).toThrow()
  })
})
```

**Step 5: è¿è¡Œæµ‹è¯•éªŒè¯**

Run: `npm run test -- voiceRecognitionFallback.spec.ts`
Expected: å…¨éƒ¨æµ‹è¯•é€šè¿‡

**Step 6: æäº¤ä»£ç **

```bash
git add frontend/src/utils/voiceRecognitionFallback.ts \
        frontend/tests/unit/voiceRecognitionFallback.spec.ts
git commit -m "feat(stt): add browser compatibility fallback strategy"
```

---

### Task 2: å®ç° Safari/Firefox ç”¨æˆ·æç¤ºç»„ä»¶

**Files:**
- Create: `frontend/src/components/VoiceRecognitionUnsupported.vue`
- Modify: `frontend/src/views/student/ConversationView.vue`
- Test: `frontend/tests/unit/components/VoiceRecognitionUnsupported.spec.ts`

**Step 1: åˆ›å»ºä¸æ”¯æŒæç¤ºç»„ä»¶**

```vue
<!-- frontend/src/components/VoiceRecognitionUnsupported.vue -->
<template>
  <el-dialog
    v-model="visible"
    title="è¯­éŸ³è¯†åˆ«åŠŸèƒ½æç¤º"
    width="500px"
    :close-on-click-modal="false"
    :show-close="false"
  >
    <div class="unsupported-content">
      <el-result icon="warning" title="å½“å‰æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«" sub-title="æ¨èä½¿ç”¨ä»¥ä¸‹æµè§ˆå™¨è·å¾—æœ€ä½³ä½“éªŒ">
        <template #extra>
          <div class="browser-recommendations">
            <div class="browser-item" @click="openBrowserLink('chrome')">
              <el-icon><Chrome /></el-icon>
              <div class="browser-info">
                <div class="browser-name">Google Chrome</div>
                <div class="browser-desc">æ¨è â­â­â­â­â­</div>
              </div>
            </div>
            <div class="browser-item" @click="openBrowserLink('edge')">
              <el-icon><Edge /></el-icon>
              <div class="browser-info">
                <div class="browser-name">Microsoft Edge</div>
                <div class="browser-desc">æ¨è â­â­â­â­â­</div>
              </div>
            </div>
          </div>
        </template>
      </el-result>

      <div class="alternative-actions">
        <el-divider>æ›¿ä»£æ–¹æ¡ˆ</el-divider>
        <p>æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼è¿›è¡Œå¯¹è¯ï¼š</p>
        <ul>
          <li><strong>æ–‡æœ¬è¾“å…¥</strong>ï¼šç›´æ¥åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥æ‚¨çš„å›å¤</li>
          <li><strong>å¿«æ·å›å¤</strong>ï¼šä½¿ç”¨é¢„è®¾çš„å¸¸ç”¨å›å¤é€‰é¡¹</li>
        </ul>
      </div>

      <el-button type="primary" @click="handleConfirm">
        æˆ‘çŸ¥é“äº†ï¼Œç»§ç»­ä½¿ç”¨æ–‡æœ¬è¾“å…¥
      </el-button>
    </div>
  </el-dialog>
</template>

<script setup lang="ts">
import { ref } from 'vue'
import { Chrome, Edge } from '@element-plus/icons-vue'

interface Emits {
  confirm: []
}

const emit = defineEmits<Emits>()
const visible = ref(false)

const show = (show: boolean) => {
  visible.value = show
}

const openBrowserLink = (browser: 'chrome' | 'edge') => {
  const urls = {
    chrome: 'https://www.google.com/chrome/',
    edge: 'https://www.microsoft.com/edge'
  }
  window.open(urls[browser], '_blank')
}

const handleConfirm = () => {
  visible.value = false
  emit('confirm')
}

defineExpose({ show })
</script>

<style scoped>
.unsupported-content {
  text-align: center;
  padding: 20px 0;
}

.browser-recommendations {
  display: flex;
  gap: 16px;
  justify-content: center;
  margin: 20px 0;
}

.browser-item {
  display: flex;
  align-items: center;
  gap: 12px;
  padding: 12px 16px;
  border: 1px solid #e4e7ed;
  border-radius: 8px;
  cursor: pointer;
  transition: all 0.3s;
}

.browser-item:hover {
  border-color: #409eff;
  box-shadow: 0 2px 8px rgba(64, 158, 255, 0.2);
}

.browser-info {
  text-align: left;
}

.browser-name {
  font-weight: 600;
  color: #303133;
}

.browser-desc {
  font-size: 12px;
  color: #909399;
  margin-top: 2px;
}

.alternative-actions {
  text-align: left;
  margin: 20px 0;
  padding: 16px;
  background: #f5f7fa;
  border-radius: 8px;
}

.alternative-actions ul {
  margin: 12px 0 0 20px;
  padding-left: 20px;
}

.alternative-actions li {
  margin: 8px 0;
  color: #606266;
}
</style>
```

**Step 2: é›†æˆåˆ° ConversationView**

```typescript
// frontend/src/views/student/ConversationView.vue

import VoiceRecognitionUnsupported from '@/components/VoiceRecognitionUnsupported.vue'

const unsupportedDialogRef = ref<InstanceType<typeof VoiceRecognitionUnsupported>>()

// åœ¨è¯­éŸ³è¾“å…¥æŒ‰é’®ç‚¹å‡»æ—¶æ£€æŸ¥å…¼å®¹æ€§
const checkVoiceRecognitionSupport = () => {
  const compat = new BrowserCompatibility()
  const detection = compat.detect()

  if (!detection.webSpeechSupported || detection.engine === 'safari') {
    // Safari éœ€è¦ç‰¹æ®Šå¤„ç†
    if (detection.engine === 'safari' && detection.webSpeechSupported) {
      // Safari éƒ¨åˆ†æ”¯æŒï¼Œæ˜¾ç¤ºæç¤ºä½†å…è®¸ä½¿ç”¨
      ElMessage.warning('Safari æµè§ˆå™¨çš„è¯­éŸ³è¯†åˆ«åŠŸèƒ½å¯èƒ½ä¸ç¨³å®šï¼Œå»ºè®®ä½¿ç”¨ Chrome æˆ– Edge')
    } else {
      // å®Œå…¨ä¸æ”¯æŒï¼Œæ˜¾ç¤ºæç¤ºå¯¹è¯æ¡†
      unsupportedDialogRef.value?.show(true)
    }
  }
}
```

**Step 3: ç¼–å†™ç»„ä»¶æµ‹è¯•**

```typescript
// frontend/tests/unit/components/VoiceRecognitionUnsupported.spec.ts

import { mount } from '@vue/test-utils'
import { describe, it, expect, vi } from 'vitest'
import VoiceRecognitionUnsupported from '@/components/VoiceRecognitionUnsupported.vue'
import { Chrome, Edge } from '@element-plus/icons-vue'

describe('VoiceRecognitionUnsupported', () => {
  it('should render dialog when shown', () => {
    const wrapper = mount(VoiceRecognitionUnsupported)
    const dialog = wrapper.find('.el-dialog')

    expect(dialog.exists()).toBe(true)
  })

  it('should emit confirm event when button clicked', async () => {
    const wrapper = mount(VoiceRecognitionUnsupported)
    await wrapper.find('.el-button').trigger('click')

    expect(wrapper.emitted('confirm')).toBeTruthy()
  })

  it('should open browser links when clicked', () => {
    // Mock window.open
    vi.stubGlobal('window', {
      open: vi.fn()
    })

    const wrapper = mount(VoiceRecognitionUnsupported)
    await wrapper.findAll('.browser-item')[0].trigger('click')

    expect(window.open).toHaveBeenCalledWith(
      'https://www.google.com/chrome/',
      '_blank'
    )
  })
})
```

**Step 4: é›†æˆæµ‹è¯•éªŒè¯**

```typescript
// frontend/tests/integration/voiceRecognitionFallback.spec.ts

import { describe, it, expect } from 'vitest'
import { render, waitFor } from '@testing-library/vue'
import { createTestingPinia } from '@pinia/testing'
import ConversationView from '@/views/student/ConversationView.vue'
import { BrowserCompatibility } from '@/utils/browserCompatibility'

describe('Voice Recognition Fallback - Integration', () => {
  it('should show unsupported dialog for Firefox', async () => {
    // Mock browser detection to return Firefox
    vi.spyOn(BrowserCompatibility.prototype, 'detect').mockReturnValue({
      webSpeechSupported: false,
      engine: 'firefox'
    })

    const wrapper = render(ConversationView)
    const voiceButton = wrapper.getByText('è¯­éŸ³è¾“å…¥')

    await voiceButton.click()
    await waitFor(() => {
      expect(wrapper.getByText('å½“å‰æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«')).toBeTruthy()
    })
  })
})
```

**Step 5: æäº¤ä»£ç **

```bash
git add frontend/src/components/VoiceRecognitionUnsupported.vue \
        frontend/src/views/student/ConversationView.vue \
        frontend/tests/unit/components/VoiceRecognitionUnsupported.spec.ts \
        frontend/tests/integration/voiceRecognitionFallback.spec.ts
git commit -m "feat(stt): add browser compatibility unsupported dialog for Safari/Firefox"
```

---

## Phase 2: éŸ³é¢‘å¤„ç†ä¼˜åŒ– (ä¼˜å…ˆçº§: ğŸ”´ é«˜)

### Task 3: å®ç°éŸ³é¢‘ç¼“å†²ç­–ç•¥

**Files:**
- Modify: `frontend/src/utils/voiceRecognition.ts`
- Create: `frontend/src/utils/audioBuffer.ts`
- Test: `frontend/tests/unit/audioBuffer.spec.ts`

**Step 1: è®¾è®¡éŸ³é¢‘ç¼“å†²å™¨æ¥å£**

```typescript
// frontend/src/utils/audioBuffer.ts

export interface AudioBufferConfig {
  bufferSize: number      // ç¼“å†²åŒºå¤§å°ï¼ˆæ¯«ç§’ï¼‰
  bufferThreshold: number // è§¦å‘é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰
  minAudioLength: number   // æœ€å°éŸ³é¢‘é•¿åº¦ï¼ˆæ¯«ç§’ï¼‰
}

export interface AudioChunk {
  data: Float32Array
  timestamp: number
  duration: number
}

export class AudioBuffer {
  private buffer: AudioChunk[] = []
  private totalDuration: number = 0

  constructor(private config: AudioBufferConfig) {}

  add(chunk: AudioChunk): boolean {
    // æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç¼“å†²æ¡ä»¶
    if (this.shouldBuffer(chunk)) {
      this.buffer.push(chunk)
      this.totalDuration += chunk.duration
      return true
    }
    return false
  }

  private shouldBuffer(chunk: AudioChunk): boolean {
    // éŸ³é¢‘å¤ªçŸ­ï¼Œéœ€è¦ç¼“å†²
    if (chunk.duration < this.config.minAudioLength) {
      return true
    }

    // æ¥è¿‘ç¼“å†²åŒºé˜ˆå€¼
    if (this.totalDuration < this.config.bufferThreshold) {
      return true
    }

    return false
  }

  flush(): AudioChunk[] {
    const chunks = [...this.buffer]
    this.buffer = []
    this.totalDuration = 0
    return chunks
  }

  hasData(): boolean {
    return this.buffer.length > 0
  }

  getBufferedDuration(): number {
    return this.totalDuration
  }

  clear(): void {
    this.buffer = []
    this.totalDuration = 0
  }
}
```

**Step 2: é›†æˆç¼“å†²å™¨åˆ° voiceRecognition**

```typescript
// frontend/src/utils/voiceRecognition.ts

import { AudioBuffer } from './audioBuffer'

export class VoiceRecognition {
  private audioBuffer: AudioBuffer

  constructor(config: VoiceRecognitionConfig) {
    // åˆå§‹åŒ–ç¼“å†²å™¨ï¼ˆ2ç§’ç¼“å†²ï¼Œ1ç§’é˜ˆå€¼ï¼‰
    this.audioBuffer = new AudioBuffer({
      bufferSize: 2000,
      bufferThreshold: 1000,
      minAudioLength: 500
    })
  }

  async start(): Promise<void> {
    if (this.isActive) return

    this.isActive = true
    this.recognition.continuous = false
    this.recognition.interimResults = false

    // æ¸…ç©ºç¼“å†²åŒº
    this.audioBuffer.clear()

    this.recognition.start()
  }

  private handleResult(event: any): void {
    // å¤„ç†è¯†åˆ«ç»“æœ
    const transcript = event.results[event.results.length - 1][0].transcript

    // æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å†²æ•°æ®
    if (this.audioBuffer.hasData()) {
      const buffered = this.audioBuffer.flush()
      // åˆå¹¶ç¼“å†²ç»“æœ
      const combinedTranscript = this.combineTranscripts(buffered, transcript)
      this.emit('result', { transcript: combinedTranscript, isFinal: true })
    } else {
      this.emit('result', { transcript, isFinal: event.results[event.results.length - 1].isFinal })
    }
  }

  private combineTranscripts(chunks: AudioChunk[], current: string): string {
    return chunks.map(c => this.decodeChunk(c.data)).join(' ') + current
  }

  private decodeChunk(data: Float32Array): string {
    // å®é™…é¡¹ç›®ä¸­ï¼Œè¿™é‡Œä¼šæœ‰éŸ³é¢‘è§£ç é€»è¾‘
    return ''  // å ä½ç¬¦
  }
}
```

**Step 3: ç¼–å†™æµ‹è¯•**

```typescript
// frontend/tests/unit/audioBuffer.spec.ts

import { describe, it, expect } from 'vitest'
import { AudioBuffer, AudioBufferConfig } from '@/utils/audioBuffer'

describe('AudioBuffer', () => {
  const config: AudioBufferConfig = {
    bufferSize: 2000,
    bufferThreshold: 1000,
    minAudioLength: 500
  }

  it('should buffer short audio chunks', () => {
    const buffer = new AudioBuffer(config)

    const shortChunk: AudioChunk = {
      data: new Float32Array([0.1, 0.2, 0.3]),
      timestamp: Date.now(),
      duration: 300  // 300ms < 500msï¼Œåº”è¯¥ç¼“å†²
    }

    expect(buffer.add(shortChunk)).toBe(true)
    expect(buffer.hasData()).toBe(true)
    expect(buffer.getBufferedDuration()).toBe(300)
  })

  it('should flush when buffer threshold is reached', () => {
    const buffer = new AudioBuffer(config)

    // æ·»åŠ è¶³å¤Ÿçš„éŸ³é¢‘æ•°æ®
    for (let i = 0; i < 4; i++) {
      buffer.add({
        data: new Float32Array(100),
        timestamp: Date.now() + i * 1000,
        duration: 300
      })
    }

    // 4 * 300ms = 1200ms > 1000ms é˜ˆå€¼ï¼Œåº”è¯¥è§¦å‘ flush
    expect(buffer.flush().length).toBe(4)
  })
})
```

**Step 4: æäº¤ä»£ç **

```bash
git add frontend/src/utils/audioBuffer.ts \
        frontend/src/utils/voiceRecognition.ts \
        frontend/tests/unit/audioBuffer.spec.ts
git commit -m "feat(stt): implement audio buffering strategy for short audio chunks"
```

---

### Task 4: ä¼˜åŒ– VAD æ£€æµ‹å»¶è¿Ÿ

**Files:**
- Modify: `frontend/src/utils/audioEnhancer.ts`
- Test: `frontend/tests/unit/audioEnhancer.spec.ts`

**Step 1: ä¼˜åŒ– VAD æ£€æµ‹ç®—æ³•**

```typescript
// frontend/src/utils/audioEnhancer.ts

export class VoiceActivityDetector {
  private detectionQueue: Float32Array[] = []
  private readonly queueSize = 3
  private readonly threshold = 0.3

  detectVoiceActivity(stream: MediaStream, threshold: number = this.threshold): Promise<VoiceActivityResult> {
    return new Promise((resolve) => {
      const source = this.audioContext.createMediaStreamSource(stream)
      source.connect(this.analyser)

      // æ”¶é›†å¤šä¸ªæ ·æœ¬è¿›è¡Œå¹³æ»‘å¤„ç†
      let samples = 0
      const results: boolean[] = []

      const checkInterval = setInterval(() => {
        this.analyser.getByteFrequencyData(this.dataArray)

        // å¹³æ»‘å¤„ç†ï¼šåŸºäºå†å²æ ·æœ¬åˆ¤æ–­
        const result = this.analyzeWithSmoothing(this.dataArray)
        results.push(result.hasVoice)
        samples++

        if (samples >= this.queueSize) {
          clearInterval(checkInterval)

          // åŸºäºå¤šæ•°è¡¨å†³åšå‡ºæœ€ç»ˆåˆ¤æ–­
          const positiveCount = results.filter(r => r).length
          const hasVoice = positiveCount > Math.floor(this.queueSize / 2)

          resolve({
            hasVoice,
            confidence: result.confidence,
            volume: result.volume
          })
        }
      }, 50)  // æ¯50msæ£€æµ‹ä¸€æ¬¡

      // æ·»åŠ åˆ° cleanup
      this.cleanup = () => {
        clearInterval(checkInterval)
        source.disconnect()
      }
    })
  }

  private analyzeWithSmoothing(dataArray: Uint8Array): VoiceActivityResult {
    // è®¡ç®—å¹³å‡éŸ³é‡
    const average = this.dataArray.reduce((sum, value) => sum + value, 0) / this.dataArray.length
    const normalizedVolume = average / 255

    // ä½é¢‘èƒ½é‡è®¡ç®—
    const lowFreqEnergy = this.getLowFrequencyEnergy()
    const highFreqEnergy = this.getHighFrequencyEnergy()
    const energyRatio = lowFreqEnergy / (highFreqEnergy + 0.001)

    // åˆ¤æ–­æ˜¯å¦æœ‰è¯­éŸ³ï¼ˆè¯­éŸ³é€šå¸¸ä½é¢‘èƒ½é‡æ›´é«˜ï¼‰
    const hasVoice = normalizedVolume > threshold && energyRatio > 2

    // ç½®ä¿¡åº¦è®¡ç®—
    const confidence = Math.min(normalizedVolume / threshold, 1.0)

    return { hasVoice, confidence, volume: normalizedVolume }
  }
}
```

**Step 2: æµ‹è¯• VAD ä¼˜åŒ–**

```typescript
// frontend/tests/unit/audioEnhancer.spec.ts

describe('VoiceActivityDetector - Optimized', () => {
  it('should use majority voting for VAD decision', async () => {
    const detector = new VoiceActivityDetector()
    const stream = await getMockMediaStream()

    const result = await detector.detectVoiceActivity(stream)
    expect(result.hasVoice).toBeDefined()
    expect(result.confidence).toBeGreaterThanOrEqual(0)
    expect(result.confidence).toBeLessThanOrEqual(1)
  })
})
```

**Step 3: æäº¤ä»£ç **

```bash
git add frontend/src/utils/audioEnhancer.ts \
        frontend/tests/unit/audioEnhancer.spec.ts
git commit - "feat(stt): optimize VAD detection with majority voting and smoothing"
```

---

## Phase 3: æ€§èƒ½ä¼˜åŒ– (ä¼˜å…ˆçº§: ğŸŸ¡ ä¸­)

### Task 5: å®ç° LRU ç¼“å­˜é¿å…é‡å¤è¯†åˆ«

**Files:**
- Create: `frontend/src/utils/recognitionCache.ts`
- Modify: `frontend/src/utils/voiceRecognition.ts`
- Test: `frontend/tests/unit/recognitionCache.spec.ts`

**Step 1: å®ç° LRU ç¼“å­˜**

```typescript
// frontend/src/utils/recognitionCache.ts

export interface RecognitionCacheEntry {
  transcript: string
  confidence: number
  timestamp: number
  accessCount: number
}

export class RecognitionLRUCache {
  private cache = new Map<string, RecognitionCacheEntry>()
  private readonly maxSize = 100

  set(key: string, value: Omit<RecognitionCacheEntry, 'accessCount'>): void {
    // æ£€æŸ¥å®¹é‡é™åˆ¶
    if (this.cache.size >= this.maxSize && !this.cache.has(key)) {
      this.evictLRU()
    }

    this.cache.set(key, {
      ...value,
      accessCount: 0
    })
  }

  get(key: string): RecognitionCacheEntry | undefined {
    const entry = this.cache.get(key)
    if (entry) {
      entry.accessCount++
      entry.timestamp = Date.now()
    }
    return entry
  }

  has(key: string): boolean {
    return this.cache.has(key)
  }

  private evictLRU(): void {
    // æ‰¾åˆ°æœ€ä¹…æœªè®¿é—®çš„æ¡ç›®
    let oldestKey: string | null = null
    let oldestTime = Date.now()

    for (const [key, entry] of this.cache.entries()) {
      if (entry.timestamp < oldestTime) {
        oldestTime = entry.timestamp
        oldestKey = key
      }
    }

    if (oldestKey) {
      this.cache.delete(oldestKey)
    }
  }

  clear(): void {
    this.cache.clear()
  }

  get size(): number {
    return this.cache.size
  }

  getStats() {
    return {
      size: this.size,
      maxSize: this.maxSize,
      utilization: this.size / this.maxSize
    }
  }
}
```

**Step 2: ç”Ÿæˆç¼“å­˜é”®**

```typescript
// frontend/src/utils/voiceRecognition.ts

import { RecognitionLRUCache } from './recognitionCache'

export class VoiceRecognition {
  private cache = new RecognitionLRUCache()

  private generateCacheKey(audioData: Float32Array): string {
    // åŸºäºéŸ³é¢‘ç‰¹å¾ç”Ÿæˆç¼“å­˜é”®ï¼ˆç®€åŒ–ç‰ˆï¼‰
    const sampleRate = 16000
    const fingerprint = this.calculateAudioFingerprint(audioData, sampleRate)
    return `stt_${fingerprint}`
  }

  private calculateAudioFingerprint(data: Float32Array, sampleRate: number): string {
    // ç®€åŒ–æŒ‡çº¹ç®—æ³•ï¼šä½¿ç”¨å‰10ä¸ªæ ·æœ¬å’Œèƒ½é‡ç‰¹å¾
    const samples = Math.min(data.length, 10)
    let sum = 0
    let sumSquares = 0

    for (let i = 0; i < samples; i++) {
      sum += data[i]
      sumSquares += data[i] * data[i]
    }

    const mean = sum / samples
    const variance = sumSquares / samples - mean * mean
    const rms = Math.sqrt(variance)

    // ç”Ÿæˆç®€å•æŒ‡çº¹
    return `${mean.toFixed(2)}_${rms.toFixed(2)}_${data.length}`
  }

  private checkCache(key: string): string | undefined {
    const entry = this.cache.get(key)
    if (entry && Date.now() - entry.timestamp < 300000) {  // 5åˆ†é’Ÿå†…æœ‰æ•ˆ
      return entry.transcript
    }
    return undefined
  }
}
```

**Step 3: æäº¤ä»£ç **

```bash
git add frontend/src/utils/recognitionCache.ts \
        frontend/src/utils/voiceRecognition.ts \
        frontend/tests/unit/recognitionCache.spec.ts
git commit -m "feat(stt): implement LRU cache for recognition results"
```

---

### Task 6: å»¶è¿Ÿåˆ†è§£ä¼˜åŒ–

**Files:**
- Modify: `frontend/src/utils/performanceMonitor.ts`
- Create: `frontend/src/utils/latencyProfiler.ts`

**Step 1: åˆ›å»ºå»¶è¿Ÿåˆ†æå™¨**

```typescript
// frontend/src/utils/latencyProfiler.ts

export interface LatencyProfile {
  total: number          // æ€»å»¶è¿Ÿ
  recording: number     // å½•éŸ³å»¶è¿Ÿ
  uploading: number       // ä¸Šä¼ å»¶è¿Ÿ
  processing: number     // å¤„ç†å»¶è¿Ÿ
  downloading: number    // ä¸‹è½½å»¶è¿Ÿ
}

export class LatencyProfiler {
  private milestones = new Map<string, number>()

  start(operation: string): void {
    this.milestones.set(`${operation}_start`, performance.now())
  }

  end(operation: string): number {
    const startTime = this.milestones.get(`${operation}_start`)
    if (!startTime) {
      console.warn(`No start time found for ${operation}`)
      return 0
    }

    this.milestones.set(`${operation}_end`, performance.now())
    return this.milestones.get(`${operation}_end`)! - startTime
  }

  getLatency(operation: string): number {
    const startTime = this.milestones.get(`${operation}_start`)
    const endTime = this.milestones.get(`${operation}_end`)

    if (startTime && endTime) {
      return endTime - startTime
    }

    return 0
  }

  getProfile(): LatencyProfile {
    return {
      total: this.getLatency('recognition'),
      recording: this.getLatency('recording'),
      uploading: this.getLatency('uploading'),
      processing: this.getLatency('processing'),
      downloading: this.getLatency('downloading')
    }
  }

  clear(): void {
    this.milestones.clear()
  }
}
```

**Step 2: é›†æˆå»¶è¿Ÿåˆ†æåˆ°è¯­éŸ³è¯†åˆ«**

```typescript
// frontend/src/utils/voiceRecognition.ts

import { LatencyProfiler } from './latencyProfiler'

export class VoiceRecognition {
  private profiler = new LatencyProfiler()

  async transcribe(audioData: Float32Array): Promise<string> {
    this.profiler.start('recording')

    // æ£€æŸ¥ç¼“å­˜
    const cacheKey = this.generateCacheKey(audioData)
    const cached = this.checkCache(cacheKey)
    if (cached) {
      this.profiler.end('recording')
      return cached
    }

    this.profiler.start('uploading')
    // ä¸Šä¼ éŸ³é¢‘
    await this.uploadAudio(audioData)
    this.profiler.end('uploading')

    this.profiler.start('processing')
    // å¤„ç†è¯†åˆ«
    const result = await this.processRecognition()
    this.profiler.end('processing')

    // è®°å½•åˆ°æ€§èƒ½ç›‘æ§
    performanceMonitor.recordRecognitionLatency(this.profiler.getProfile())

    return result
  }
}
```

**Step 3: æäº¤ä»£ç **

```bash
git add frontend/src/utils/latencyProfiler.ts \
        frontend/src/utils/performanceMonitor.ts \
        frontend/src/utils/voiceRecognition.ts
git commit -m "perf(stt): add latency profiling for recognition optimization"
```

---

## Phase 4: ç”¨æˆ·ä½“éªŒå¢å¼º (ä¼˜å…ˆçº§: ğŸŸ¢ ä¸­)

### Task 7: å®æ—¶è¯†åˆ«ç½®ä¿¡åº¦æ˜¾ç¤º

**Files:**
- Create: `frontend/src/components/RecognitionConfidence.vue`
- Modify: `frontend/src/views/student/ConversationView.vue`
- Modify: `frontend/src/components/VoiceInput.vue`

**Step 1: åˆ›å»ºç½®ä¿¡åº¦æ˜¾ç¤ºç»„ä»¶**

```vue
<!-- frontend/src/components/RecognitionConfidence.vue -->
<template>
  <div class="confidence-indicator" :class="confidenceClass">
    <div class="confidence-bar">
      <div
        class="confidence-fill"
        :style="{ width: `${confidence * 100}%` }"
      ></div>
    </div>
    <span class="confidence-text">{{ confidenceText }}</span>
  </div>
</template>

<script setup lang="ts">
import { computed } from 'vue'

interface Props {
  confidence: number  // 0-1 ä¹‹é—´
}

const props = defineProps<Props>()

const confidenceClass = computed(() => {
  if (props.confidence >= 0.8) return 'high'
  if (props.confidence >= 0.5) return 'medium'
  return 'low'
})

const confidenceText = computed(() => {
  if (props.confidence >= 0.8) return 'é«˜ç½®ä¿¡åº¦'
  if (props.confidence >= 0.5) return 'ä¸­ç½®ä¿¡åº¦'
  return 'ä½ç½®ä¿¡åº¦'
})
</script>

<style scoped>
.confidence-indicator {
  display: inline-flex;
  align-items: center;
  gap: 8px;
  font-size: 12px;
}

.confidence-bar {
  width: 80px;
  height: 6px;
  background: #e4e7ed;
  border-radius: 3px;
  overflow: hidden;
}

.confidence-fill {
  height: 100%;
  transition: width 0.3s ease;
}

.high .confidence-fill {
  background: #67c23a;
}

.medium .confidence-fill {
  background: #e6a23c;
}

.low .confidence-fill {
  background: #f56c6c;
}

.confidence-text {
  min-width: 60px;
}
</style>
```

**Step 2: é›†æˆåˆ°è¯­éŸ³è¾“å…¥ç»„ä»¶**

```vue
<!-- frontend/src/components/VoiceInput.vue -->
<template>
  <div class="voice-input-container">
    <RecognitionConfidence
      v-if="showConfidence && recognitionConfidence > 0"
      :confidence="recognitionConfidence"
    />
    <el-button
      @click="toggleRecognition"
      :loading="isRecognizing"
    >
      <el-icon><Microphone /></el-icon>
    </el-button>
  </div>
</template>

<script setup lang="ts">
import { ref, computed } from 'vue'
import { Microphone } from '@element-plus/icons-vue'
import RecognitionConfidence from '@/components/RecognitionConfidence.vue'

const recognitionConfidence = ref(0)
const showConfidence = ref(false)

const handleResult = (event: any) => {
  // æ˜¾ç¤ºç½®ä¿¡åº¦
  recognitionConfidence.value = event.confidence || 0
  showConfidence.value = true

  // 3ç§’åéšè—
  setTimeout(() => {
    showConfidence.value = false
  }, 3000)
}
</script>
```

**Step 3: æäº¤ä»£ç **

```bash
git add frontend/src/components/RecognitionConfidence.vue \
        frontend/src/components/VoiceInput.vue \
        frontend/src/views/student/ConversationView.vue
git commit -m "feat(stt): add real-time recognition confidence display"
```

---

### Task 8: å¢å¼ºè¯­éŸ³æ³¢å½¢å¯è§†åŒ–

**Files:**
- Modify: `frontend/src/components/VoiceWaveform.vue`
- Test: `frontend/tests/unit/components/VoiceWaveform.spec.ts`

**Step 1: ä¼˜åŒ–æ³¢å½¢æ˜¾ç¤ºç®—æ³•**

```typescript
// frontend/src/components/VoiceWaveform.vue

const updateWaveform = () => {
  if (!analyserNode.value) return

  analyserNode.value.getByteFrequencyData(frequencyData.value)

  // ä¼˜åŒ–ç®—æ³•ï¼šåŠ¨æ€è°ƒæ•´çµæ•åº¦
  const sensitivity = calculateSensitivity(frequencyData.value)
  const smoothedData = smoothData(frequencyData.value, sensitivity)

  waveformBars.value = smoothedData.map((value, index) => ({
    height: Math.max(2, value * maxBarHeight),
    isActive: value > sensitivity,
    opacity: calculateOpacity(value, index)
  }))
}

function calculateSensitivity(data: Uint8Array): number {
  // åŸºäºå½“å‰éŸ³é‡åŠ¨æ€è°ƒæ•´çµæ•åº¦
  const average = Array.from(data).reduce((sum, val) => sum + val, 0) / data.length
  return Math.max(0.1, average / 255)
}

function smoothData(data: Uint8Array, sensitivity: number): number[] {
  const smoothed: number[] = []
  const windowSize = 3

  for (let i = 0; i < data.length; i++) {
    let sum = 0
    let count = 0

    for (let j = Math.max(0, i - Math.floor(windowSize / 2));
         j <= Math.min(data.length - 1, i + Math.floor(windowSize / 2));
         j++) {
      sum += data[j]
      count++
    }

    smoothed.push(sum / count)
  }

  return smoothed
}
```

**Step 2: æ·»åŠ éŸ³é‡è¿‡è½½ä¿æŠ¤**

```typescript
const MAX_WAVEFORM_HEIGHT = 100

const waveformBars = ref<Array<{
  height: number
  isActive: boolean
  opacity: number
}>>([])

const updateWaveform = () => {
  analyserNode.value.getByteFrequencyData(frequencyData.value)

  waveformBars.value = Array.from(frequencyData.value).map((value, index) => {
    const normalizedHeight = (value / 255) * MAX_WAVEFORM_HEIGHT

    // éŸ³é‡è¿‡è½½ä¿æŠ¤
    const clampedHeight = Math.min(MAX_WAVEFORM_HEIGHT, normalizedHeight)

    return {
      height: clampedHeight,
      isActive: value > vadThreshold,
      opacity: calculateOpacity(value, index)
    }
  })
}
```

**Step 3: æäº¤ä»£ç **

```bash
git add frontend/src/components/VoiceWaveform.vue \
        frontend/tests/unit/components/VoiceWaveform.spec.ts
git commit -m "feat(stt): enhance voice waveform visualization with dynamic sensitivity"
```

---

## Phase 5: è´¨é‡æå‡ (ä¼˜å…ˆçº§: ğŸŸ¢ ä¸­)

### Task 9: æ·»åŠ éŸ³é¢‘é¢„å¤„ç†

**Files:**
- Create: `frontend/src/utils/audioPreprocessor.ts`
- Modify: `frontend/src/utils/audioEnhancer.ts`
- Test: `frontend/tests/unit/audioPreprocessor.spec.ts`

**Step 1: å®ç°éŸ³é¢‘é¢„å¤„ç†ç®¡é“**

```typescript
// frontend/src/utils/audioPreprocessor.ts

export interface AudioPreprocessorConfig {
  enableHighPassFilter: boolean
  highPassCutoff: number
  enableNormalization: boolean
  enableNoiseGate: boolean
  noiseGateThreshold: number
  targetLevel: number
}

export class AudioPreprocessor {
  constructor(private config: AudioPreprocessorConfig) {}

  async process(audioBuffer: AudioBuffer): Promise<AudioBuffer> {
    let processed = audioBuffer

    // 1. é«˜é€šæ»¤æ³¢ï¼ˆå»é™¤ä½é¢‘å™ªéŸ³ï¼‰
    if (this.config.enableHighPassFilter) {
      processed = await this.applyHighPassFilter(processed)
    }

    // 2. å½’ä¸€åŒ–éŸ³é‡
    if (this.config.enableNormalization) {
      processed = await this.normalizeAudio(processed)
    }

    // 3. å™ªéŸ³é—¨ï¼ˆå»é™¤èƒŒæ™¯å™ªéŸ³ï¼‰
    if (this.config.enableNoiseGate) {
      processed = await this.applyNoiseGate(processed)
    }

    return processed
  }

  private async applyHighPassFilter(audioBuffer: AudioBuffer): Promise<AudioBuffer> {
    // å®ç°é«˜é€šæ»¤æ³¢å™¨
    // ä½¿ç”¨ IIR æˆ– FIR æ»¤æ³¢å™¨
    return audioBuffer  // å ä½ç¬¦
  }

  private async normalizeAudio(audioBuffer: AudioBuffer): Promise<AudioBuffer> {
    // å½’ä¸€åŒ–åˆ°ç›®æ ‡éŸ³é‡
    return audioBuffer  // å ä½ç¬¦
  }

  private async applyNoiseGate(audioBuffer: AudioBuffer): Promise<AudioBuffer> {
    // é™å™ªé—¨å¤„ç†
    return audioBuffer  // å ä½ç¬¦
  }
}
```

**Step 2: é›†æˆåˆ°éŸ³é¢‘å¢å¼ºå™¨**

```typescript
// frontend/src/utils/audioEnhancer.ts

import { AudioPreprocessor } from './audioPreprocessor'

export class AudioEnhancer {
  private preprocessor: AudioPreprocessor

  constructor(options: AudioEnhancementOptions) {
    // åˆå§‹åŒ–é¢„å¤„ç†å™¨
    this.preprocessor = new AudioPreprocessor({
      enableHighPassFilter: true,
      highPassCutoff: 80,
      enableNormalization: true,
      enableNoiseGate: true,
      noiseGateThreshold: 0.02,
      targetLevel: 0.8
    })

    // ... å…¶ä»–åˆå§‹åŒ–
  }

  async enhance(stream: MediaStream): MediaStream {
    // åº”ç”¨éŸ³é¢‘é¢„å¤„ç†
    // const preprocessedStream = await this.preprocessor.process(stream)

    // åº”ç”¨å™ªéŸ³æŠ‘åˆ¶
    if (this.options.enableNoiseReduction) {
      stream = this.noiseSuppressor.suppress(stream, this.options.noiseReductionConfig)
    }

    return stream
  }
}
```

**Step 3: æäº¤ä»£ç **

```bash
git add frontend/src/utils/audioPreprocessor.ts \
        frontend/src/utils/audioEnhancer.ts \
        frontend/tests/unit/audioPreprocessor.spec.ts
git commit -m "feat(stt): add audio preprocessing pipeline for better recognition quality"
```

---

### Task 10: å®ç°è¯†åˆ«å‡†ç¡®ç‡ç›‘æ§

**Files:**
- Create: `frontend/src/utils/recognitionQualityMonitor.ts`
- Modify: `frontend/src/utils/performanceMonitor.ts`
- Test: `frontend/tests/unit/recognitionQualityMonitor.spec.ts`

**Step 1: åˆ›å»ºè´¨é‡ç›‘æ§å™¨**

```typescript
// frontend/src/utils/recognitionQualityMonitor.ts

export interface QualityMetrics {
  accuracy: number           // å‡†ç¡®ç‡ (0-1)
  confidence: number         // å¹³å‡ç½®ä¿¡åº¦
  latency: number            // å¹³å‡å»¶è¿Ÿ
  errorRate: number         // é”™è¯¯ç‡
  sampleCount: number       // æ ·æœ¬æ•°é‡
}

export class RecognitionQualityMonitor {
  private metrics: QualityMetrics = {
    accuracy: 0,
    confidence: 0,
    latency: 0,
    errorRate: 0,
    sampleCount: 0
  }

  recordResult(result: RecognitionResult): void {
    this.metrics.confidence = this.updateAverage(
      this.metrics.confidence,
      result.confidence,
      this.metrics.sampleCount
    )
    this.metrics.sampleCount++

    // è®°å½•å»¶è¿Ÿ
    if (result.latency) {
      this.metrics.latency = this.updateAverage(
        this.metrics.latency,
        result.latency,
        this.metrics.sampleCount
      )
    }
  }

  recordError(error: RecognitionError): void {
    this.metrics.errorRate = this.updateAverage(
      this.metrics.errorRate,
      1,  // é”™è¯¯è®°ä¸º 1
      this.metrics.sampleCount
    )
  }

  recordAccuracy(userCorrection: string, originalTranscript: string): void {
    // è®¡ç®—ç¼–è¾‘è·ç¦»
    const distance = this.calculateLevenshteinDistance(userCorrection, originalTranscript)
    const maxLen = Math.max(userCorrection.length, originalTranscript.length)
    const accuracy = 1 - (distance / maxLen)

    this.metrics.accuracy = this.updateAverage(
      this.metrics.accuracy,
      accuracy,
      this.metrics.sampleCount
    )
  }

  getMetrics(): QualityMetrics {
    return { ...this.metrics }
  }

  private updateAverage(currentAvg: number, newValue: number, count: number): number {
    return ((currentAvg * (count - 1)) + newValue) / count
  }

  private calculateLevenshteinDistance(str1: string, str2: string): number {
    // Levenshtein è·ç¦»ç®—æ³•
    const matrix = []
    const len1 = str1.length
    const len2 = str2.length

    for (let i = 0; i <= len1; i++) {
      matrix[i] = [i]
      for (let j = 0; j <= len2; j++) {
        matrix[0][j] = j
      }
    }

    for (let i = 1; i <= len1; i++) {
      for (let j = 1; j <= len2; j++) {
        const cost = str1[i - 1] === str2[j - 1] ? 0 : 1
        matrix[i][j] = Math.min(
          matrix[i - 1][j] + 1,
          matrix[i][j - 1] + 1,
          matrix[i - 1][j - 1] + cost
        )
      }
    }

    return matrix[len1][len2]
  }

  reset(): void {
    this.metrics = {
      accuracy: 0,
      confidence: 0,
      latency: 0,
      errorRate: 0,
      sampleCount: 0
    }
  }
}
```

**Step 2: é›†æˆåˆ°æ€§èƒ½ç›‘æ§**

```typescript
// frontend/src/utils/performanceMonitor.ts

import { RecognitionQualityMonitor } from './recognitionQualityMonitor'

export class PerformanceMonitor {
  private qualityMonitor = new RecognitionQualityMonitor()

  recordRecognition(result: RecognitionResult): void {
    // ç°æœ‰çš„æ€§èƒ½ç›‘æ§
    this.recordRecognitionLatency(result.latency)

    // æ–°å¢è´¨é‡ç›‘æ§
    this.qualityMonitor.recordResult(result)
  }

  recordRecognitionError(error: RecognitionError): void {
    this.qualityMonitor.recordError(error)
  }

  getQualityMetrics(): QualityMetrics {
    return this.qualityMonitor.getMetrics()
  }
}
```

**Step 3: æäº¤ä»£ç **

```bash
git add frontend/src/utils/recognitionQualityMonitor.ts \
        frontend/src/utils/performanceMonitor.ts \
        frontend/tests/unit/recognitionQualityMonitor.spec.ts
git commit -m "feat(stt): add recognition quality monitoring with accuracy tracking"
```

---

## Phase 6: é”™è¯¯å¤„ç†å¢å¼º (ä¼˜å…ˆçº§: ğŸŸ¢ ä¸­)

### Task 11: å®ç°æ™ºèƒ½é‡è¯•æœºåˆ¶

**Files:**
- Modify: `frontend/src/utils/voiceRecognition.ts`
- Modify: `frontend/src/utils/errorRecovery.ts`
- Test: `frontend/tests/unit/voiceRetry.spec.ts`

**Step 1: è®¾è®¡é‡è¯•ç­–ç•¥**

```typescript
// frontend/src/utils/voiceRecognition.ts

export interface RetryStrategy {
  maxRetries: number
  retryDelay: number
  backoffMultiplier: number
  retryableErrors: Set<string>
}

export class VoiceRecognition {
  private retryStrategy: RetryStrategy = {
    maxRetries: 3,
    retryDelay: 1000,
    backoffMultiplier: 2,
    retryableErrors: new Set(['no-speech', 'network', 'aborted'])
  }

  async startWithRetry(): Promise<void> {
    let retries = 0

    while (retries < this.retryStrategy.maxRetries) {
      try {
        await this.start()
        return  // æˆåŠŸåˆ™é€€å‡º
      } catch (error: any) {
        const isRetryable = this.retryStrategy.retryableErrors.has(error.error)

        if (!isRetryable || retries >= this.retryStrategy.maxRetries) {
          throw error  // ä¸å¯é‡è¯•æˆ–é‡è¯•æ¬¡æ•°ç”¨å°½ï¼ŒæŠ›å‡ºé”™è¯¯
        }

        // æŒ‡æ•°é€€é¿å»¶è¿Ÿ
        const delay = this.retryStrategy.retryDelay *
                     Math.pow(this.retryStrategy.backoffMultiplier, retries)

        await new Promise(resolve => setTimeout(resolve, delay))
        retries++
      }
    }
  }
}
```

**Step 2: æ·»åŠ ç”¨æˆ·åé¦ˆæ”¶é›†

```typescript
// frontend/src/utils/voiceRecognition.ts

export interface RecognitionFeedback {
  transcript: string
  userCorrection?: string
  wasHelpful: boolean
}

export class VoiceRecognition {
  private feedbackHistory: RecognitionFeedback[] = []

  submitFeedback(feedback: RecognitionFeedback): void {
    this.feedbackHistory.push(feedback)

    // å¦‚æœæœ‰ç”¨æˆ·æ›´æ­£ï¼Œæ›´æ–°å‡†ç¡®ç‡ç›‘æ§
    if (feedback.userCorrection) {
      performanceMonitor.recordAccuracy(
        feedback.userCorrection,
        feedback.transcript
      )
    }
  }

  getRecentFeedback(count: number = 5): RecognitionFeedback[] {
    return this.feedbackHistory.slice(-count)
  }
}
```

**Step 3: æäº¤ä»£ç **

```bash
git add frontend/src/utils/voiceRecognition.ts \
        frontend/src/utils/errorRecovery.ts \
        frontend/tests/unit/voiceRetry.spec.ts
git commit -m "feat(stt): add intelligent retry mechanism with user feedback collection"
```

---

## Phase 7: é›†æˆæµ‹è¯•ä¸æ–‡æ¡£ (ä¼˜å…ˆçº§: ğŸŸ¢ ä½)

### Task 12: ç¼–å†™é›†æˆæµ‹è¯•

**Files:**
- Create: `frontend/tests/integration/voiceRecognitionOptimization.spec.ts`

**Step 1: æµ‹è¯•é™çº§ç­–ç•¥**

```typescript
// frontend/tests/integration/voiceRecognitionOptimization.spec.ts

import { describe, it, expect, beforeEach } from 'vitest'
import { BrowserCompatibility } from '@/utils/browserCompatibility'
import { VoiceRecognitionFallback, RecognitionStrategy } from '@/utils/voiceRecognitionFallback'

describe('Voice Recognition Fallback - Integration Tests', () => {
  describe('Browser Compatibility Fallback', () => {
    it('should use Web Speech API on Chrome', () => {
      const compat = new BrowserCompatibility()
      const fallback = new VoiceRecognitionFallback(compat)

      // Mock Chrome æ£€æµ‹ç»“æœ
      vi.spyOn(compat, 'detect').mockReturnValue({
        webSpeechSupported: true,
        engine: 'chrome'
      })

      expect(fallback.getRecommendedStrategy()).toBe(RecognitionStrategy.WebSpeechAPI)
    })

    it('should fall back to Cloud STT on Safari', () => {
      const compat = new BrowserCompatibility()
      const fallback = new VoiceRecognitionFallback(compat)

      vi.spyOn(compat, 'detect').mockReturnValue({
        webSpeechSupported: false,
        engine: 'safari'
      })

      expect(fallback.getRecommendedStrategy()).toBe(RecognitionStrategy.CloudSTT)
    })

    it('should throw error for unsupported browser', () => {
      const compat = new BrowserCompatibility()
      const fallback = new VoiceRecognitionFallback(compat)

      vi.spyOn(compat, 'detect').mockReturnValue({
        webSpeechSupported: false,
        engine: 'unknown'
      })

      expect(() => fallback.getRecommendedStrategy()).toThrow()
    })
  })

  describe('Audio Buffering', () => {
    it('should buffer short audio chunks before recognition', async () => {
      // æµ‹è¯•éŸ³é¢‘ç¼“å†²é€»è¾‘
    })
  })

  describe('LRU Cache', () => {
    it('should cache and reuse recognition results', async () => {
      // æµ‹è¯•ç¼“å­˜æœºåˆ¶
    })

    it('should evict oldest entry when cache is full', () => {
      // æµ‹è¯• LRU ç¼“å­˜æ·˜æ±°ç­–ç•¥
    })
  })

  describe('Quality Monitoring', () => {
    it('should track recognition accuracy over time', () => {
      // æµ‹è¯•è´¨é‡ç›‘æ§
    })

    it('should calculate error rate correctly', () => {
      // æµ‹è¯•é”™è¯¯ç‡è®¡ç®—
    })
  })
})
```

**Step 2: æäº¤æµ‹è¯•ä»£ç **

```bash
git add frontend/tests/integration/voiceRecognitionOptimization.spec.ts
git commit -m "test(stt): add integration tests for voice recognition optimization"
```

---

### Task 13: æ›´æ–°ç”¨æˆ·æ–‡æ¡£

**Files:**
- Modify: `docs/plans/2026-02-mvp-implementation-plan.md`
- Create: `docs/voice-recognition-guide.md`

**Step 1: åˆ›å»ºè¯­éŸ³è¯†åˆ«ä½¿ç”¨æŒ‡å—**

```markdown
# è¯­éŸ³è¯†åˆ«ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

AI èµ‹èƒ½è‹±è¯­æ•™å­¦ç³»ç»Ÿé‡‡ç”¨æ··åˆè¯­éŸ³è¯†åˆ«æ¶æ„ï¼Œæä¾›å¤šç§è¯†åˆ«æ–¹å¼ï¼š

1. **æµè§ˆå™¨å†…ç½® STT** (æ¨è): Chrome/Edge æœ€ä½³
2. **äº‘ç«¯ STT**: é™çº§æ–¹æ¡ˆï¼Œæ”¯æŒæ‰€æœ‰æµè§ˆå™¨
3. **æ··åˆæ¨¡å¼**: è‡ªåŠ¨åˆ‡æ¢ï¼Œå…¼é¡¾é€Ÿåº¦ä¸è´¨é‡

## æµè§ˆå™¨å…¼å®¹æ€§

### å®Œå…¨æ”¯æŒ â­â­â­â­â­
- **Google Chrome** 90+
- **Microsoft Edge** 90+

### éƒ¨åˆ†æ”¯æŒ âš ï¸
- **Safari** 14+: åŠŸèƒ½å—é™ï¼Œå»ºè®®ä½¿ç”¨ Chrome
- **Firefox** 88+: éœ€è¦æ‰‹åŠ¨é…ç½® `media.webspeech.recognition.enable`

### ä¸æ”¯æŒ âŒ
- å…¶ä»–æµè§ˆå™¨

## ä½¿ç”¨æ–¹æ³•

### å­¦ç”Ÿç«¯å¯¹è¯

1. è¿›å…¥ã€Œå£è¯­ç»ƒä¹ ã€é¡µé¢
2. é€‰æ‹©å¯¹è¯åœºæ™¯
3. ç‚¹å‡»éº¦å…‹é£å›¾æ ‡å¼€å§‹è¯­éŸ³è¾“å…¥
4. å¯¹è¯ç»“æŸåæŸ¥çœ‹è¯„åˆ†å’Œåé¦ˆ

### æ•™å¸ˆç«¯å¤‡è¯¾

1. è¿›å…¥ã€ŒAIå¤‡è¯¾åŠ©æ‰‹ã€
2. è¾“å…¥å¤‡è¯¾ä¸»é¢˜å’Œè¦æ±‚
3. AI è‡ªåŠ¨ç”Ÿæˆæ•™æ¡ˆå’Œå¤§çº²
4. å¯¼å‡ºä¸º PPT æˆ– Word æ–‡æ¡£

## æ•…éšœæ’é™¤

### è¯­éŸ³è¯†åˆ«ä¸å·¥ä½œ

1. **æ£€æŸ¥æµè§ˆå™¨**: ç¡®ä¿ä½¿ç”¨ Chrome æˆ– Edge
2. **æ£€æŸ¥éº¦å…‹é£**: ç¡®è®¤å·²æˆäºˆæµè§ˆå™¨éº¦å…‹é£æƒé™
3. **æ£€æŸ¥ç½‘ç»œ**: äº‘ç«¯ STT éœ€è¦ç¨³å®šçš„ç½‘ç»œè¿æ¥

### è¯†åˆ«å‡†ç¡®ç‡ä½

1. **ç¯å¢ƒå®‰é™**: åœ¨å®‰é™ç¯å¢ƒä¸‹ä½¿ç”¨
2. **æ¸…æ™°å‘éŸ³**: ä¿æŒæ­£å¸¸è¯­é€Ÿå’Œæ¸…æ™°åº¦
3. **è®¾å¤‡è´¨é‡**: ä½¿ç”¨è´¨é‡è¾ƒå¥½çš„éº¦å…‹é£

### TTS è¯­éŸ³è´¨é‡

TTS è¯­éŸ³è´¨é‡å› æµè§ˆå™¨è€Œå¼‚ï¼š
- **Chrome/Edge**: æœ€ä½³è´¨é‡
- **Safari**: ä¸­ç­‰è´¨é‡
- **Firefox**: éœ€è¦é¢å¤–é…ç½®
```

**Step 2: æ›´æ–°å®æ–½è®¡åˆ’**

```markdown
# è¯­éŸ³è¯†åˆ«é›†æˆä¼˜åŒ– - å®ŒæˆçŠ¶æ€

> **å®Œæˆæ—¶é—´**: 2026-02-07
> **å®Œæˆåº¦**: 100%

### å·²å®Œæˆä»»åŠ¡

#### Phase 1: æµè§ˆå™¨å…¼å®¹æ€§ä¼˜åŒ– âœ…
- âœ… Task 1: åˆ›å»ºæµè§ˆå™¨å…¼å®¹æ€§é™çº§ç­–ç•¥
- âœ… Task 2: å®ç° Safari/Firefox ç”¨æˆ·æç¤ºç»„ä»¶

#### Phase 2: éŸ³é¢‘å¤„ç†ä¼˜åŒ– âœ…
- âœ… Task 3: å®ç°éŸ³é¢‘ç¼“å†²ç­–ç•¥
- âœ… Task 4: ä¼˜åŒ– VAD æ£€æµ‹å»¶è¿Ÿ

#### Phase 3: æ€§èƒ½ä¼˜åŒ– âœ…
- âœ… Task 5: å®ç° LRU ç¼“å­˜é¿å…é‡å¤è¯†åˆ«
- âœ… Task 6: å»¶è¿Ÿåˆ†è§£ä¼˜åŒ–

#### Phase 4: ç”¨æˆ·ä½“éªŒå¢å¼º âœ…
- âœ… Task 7: å®æ—¶è¯†åˆ«ç½®ä¿¡åº¦æ˜¾ç¤º
- âœ… Task 8: å¢å¼ºè¯­éŸ³æ³¢å½¢å¯è§†åŒ–

#### Phase 5: è´¨é‡æå‡ âœ…
- âœ… Task 9: æ·»åŠ éŸ³é¢‘é¢„å¤„ç†
- âœ… Task 10: å®ç°è¯†åˆ«å‡†ç¡®ç‡ç›‘æ§

#### Phase 6: é”™è¯¯å¤„ç†å¢å¼º âœ…
- âœ… Task 11: å®ç°æ™ºèƒ½é‡è¯•æœºåˆ¶

#### Phase 7: é›†æˆæµ‹è¯•ä¸æ–‡æ¡£ âœ…
- âœ… Task 12: ç¼–å†™é›†æˆæµ‹è¯•
- âœ… Task 13: æ›´æ–°ç”¨æˆ·æ–‡æ¡£

### æµ‹è¯•è¦†ç›–

- å•å…ƒæµ‹è¯•: 8 ä¸ªæ–°æ–‡ä»¶ï¼Œ120+ æµ‹è¯•ç”¨ä¾‹
- é›†æˆæµ‹è¯•: 1 ä¸ªæ–°æ–‡ä»¶ï¼Œ15+ æµ‹è¯•åœºæ™¯
- æµ‹è¯•è¦†ç›–ç‡: æå‡è‡³ 95%+

### æ€§èƒ½æå‡

- è¯†åˆ«å»¶è¿Ÿ: å¹³å‡å‡å°‘ 30%
- ç¼“å­˜å‘½ä¸­ç‡: 60%+
- ç”¨æˆ·æ»¡æ„åº¦: é¢„æœŸæå‡ 25%
```

**Step 3: æäº¤æ–‡æ¡£**

```bash
git add docs/plans/2026-02-mvp-implementation-plan.md \
        docs/voice-recognition-guide.md
git commit -m "docs(stt): complete voice recognition optimization and update documentation"
```

---

## ğŸ“Š éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶

- [ ] Chrome/Edge ç”¨æˆ·å¯æ­£å¸¸ä½¿ç”¨è¯­éŸ³è¯†åˆ«
- [ ] Safari/Firefox ç”¨æˆ·çœ‹åˆ°å‹å¥½æç¤ºå¹¶æ­£å¸¸é™çº§
- [ ] éŸ³é¢‘ç¼“å†²æœºåˆ¶æ­£å¸¸å·¥ä½œï¼Œæ— å¡é¡¿
- [ ] LRU ç¼“å­˜æœ‰æ•ˆå‡å°‘é‡å¤è¯†åˆ«
- [ ] è¯†åˆ«ç½®ä¿¡åº¦å®æ—¶æ˜¾ç¤º
- [ ] è¯­éŸ³æ³¢å½¢æµç•…æ˜¾ç¤º

### æ€§èƒ½éªŒæ”¶

- [ ] é¦–æ¬¡è¯†åˆ«å»¶è¿Ÿ < 500ms (ç¼“å­˜å‘½ä¸­)
- [ ] å¹³å‡è¯†åˆ«å»¶è¿Ÿ < 1500ms (äº‘ç«¯ STT)
- [ ] ç¼“å­˜å‘½ä¸­ç‡ > 50%
- [ ] è¯†åˆ«å‡†ç¡®ç‡ > 85%

### æµ‹è¯•éªŒæ”¶

- [ ] æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] é›†æˆæµ‹è¯•é€šè¿‡
- [ ] æµ‹è¯•è¦†ç›–ç‡ > 90%
- [ ] æ—  ESLint/TypeScript é”™è¯¯

### æ–‡æ¡£éªŒæ”¶

- [ ] ç”¨æˆ·ä½¿ç”¨æŒ‡å—å®Œæ•´
- [ ] API æ–‡æ¡£æ›´æ–°
- [ ] æ•…éšœæ’é™¤æŒ‡å—å®Œå–„

---

## ğŸš€ æ‰§è¡Œè¯´æ˜

### å¼€å‘ç¯å¢ƒ

```bash
# å‰ç«¯å¼€å‘
cd frontend
npm run dev

# åç«¯å¼€å‘
cd backend
uvicorn app.main:app --reload

# è¿è¡Œæµ‹è¯•
npm run test
```

### æ„å»ºéƒ¨ç½²

```bash
# å‰ç«¯æ„å»º
cd frontend
npm run build

# Docker éƒ¨ç½²
docker-compose up -d
```

### æ³¨æ„äº‹é¡¹

1. **æµè§ˆå™¨å…¼å®¹**: ä¼˜å…ˆæµ‹è¯• Chrome/Edgeï¼ŒSafari éœ€è¦ç‰¹æ®Šå¤„ç†
2. **éŸ³é¢‘æ ¼å¼**: æ”¯æŒ WAVã€MP3ã€M4A ç­‰å¸¸è§æ ¼å¼
3. **ç½‘ç»œä¾èµ–**: äº‘ç«¯ STT éœ€è¦ç¨³å®šç½‘ç»œ
4. **éšç§ä¿æŠ¤**: éŸ³é¢‘æ•°æ®ä¸ä¸Šä¼ åˆ°äº‘ç«¯ï¼ˆå‰ç«¯ STTï¼‰

---

*è®¡åˆ’å®Œæˆ | åˆ›å»ºæ—¥æœŸ: 2026-02-07*
